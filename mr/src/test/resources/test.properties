# default settings for Hadoop test

#test.disable.local.es=true
# ES
#es.nodes=192.168.1.50
es.port=9500
es.batch.size.bytes=1kb
#es.nodes.client.only=true

# put pressure on the bulk API
es.batch.size.entries=3
es.batch.write.retry.wait=1s

mapred.max.split.size=134217728
#mapreduce.local.map.tasks.maximum=10

# M&R
fs.default.name=file:///
mapred.job.tracker=local

#Hive - only works if the HDFS is setup as well
hive=local

#fs.default.name=hdfs://sandbox:8020
#mapred.job.tracker=sandbox:50300

#es.net.proxy.http.host=localhost
#es.net.proxy.http.port=8080
#es.net.proxy.http.user=test
#es.net.proxy.http.pass=test

# Cascading
cascading.update.skip=true

# Pig
pig.script.info.enabled=false

# Storm
es.storm.bolt.flush.entries.size=1
