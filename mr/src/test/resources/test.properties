# default settings for Hadoop test

#test.disable.local.es=true
# ES
#es.nodes=localhost
#es.nodes=http://84f306619beadeb16048ae2d4d652ac5.eu-west-1.aws.found.io:9200/
# by default this is set dynamically based on the port used by the embedded ES instance
#es.port=9200
#es.port=9500
es.batch.size.bytes=1kb
#es.nodes.client.only=true
#es.nodes.wan.only=true

# put pressure on the bulk API
es.batch.size.entries=3
es.batch.write.retry.wait=1s

mapred.max.split.size=134217728
#mapreduce.local.map.tasks.maximum=10

# M&R
fs.default.name=file:///
mapred.job.tracker=local

#Hive - only works if the HDFS is setup as well
hive=local

#fs.default.name=hdfs://sandbox:8020
#mapred.job.tracker=sandbox:50300

#es.net.proxy.http.host=localhost
#es.net.proxy.http.port=8080
#es.net.proxy.http.user=test
#es.net.proxy.http.pass=test

# Cascading
cascading.update.skip=true

# Pig
pig.script.info.enabled=false

# Storm
es.storm.bolt.flush.entries.size=1

# Spark props
spark.master=local
spark.ui.enabled=false
spark.sql.warehouse.dir=/tmp/spark
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.executor.extraJavaOptions=-XX:MaxPermSize=256m