description = 'Elasticsearch Hadoop'

buildscript {
    repositories {
		mavenCentral()
        maven { url 'http://dl.bintray.com/content/aalmiray/asciidoctor' }
        maven { url 'http://jcenter.bintray.com' }
        maven { url 'http://repo.springsource.org/plugins-release' }

    }
    dependencies {
		classpath('org.asciidoctor:asciidoctor-gradle-plugin:0.5.0')
        classpath('org.springframework.build.gradle:propdeps-plugin:0.0.5')
		classpath('nl.javadude.gradle.plugins:license-gradle-plugin:0.5.0')
	}
}

allprojects {
	repositories {
	  mavenCentral()
	  maven { url "http://oss.sonatype.org/content/groups/public/" }
	}
}

allprojects {
	apply plugin: "java"
	apply plugin: 'eclipse'
	apply plugin: 'idea'
	apply plugin: 'propdeps'
	apply plugin: 'propdeps-idea'
	apply plugin: 'propdeps-eclipse'
	// report plugins
	apply plugin: 'findbugs'
	apply plugin: 'pmd'
	apply plugin: 'jacoco'
	apply from: "$rootDir/maven.gradle"
	apply plugin: 'license'
}

apply plugin: 'asciidoctor'

ext.hadoopClient = []
ext.hadoopDistro = project.hasProperty("distro") ? project.getProperty("distro") : "hadoopStable"

// Hadoop aliases
def hadoopStableVersion = hadoop12Version
def hadoopYarnVersion = hadoop2Version
def hadoopVersion = hadoopStableVersion 
def hadoopMsg;

switch (hadoopDistro) {

  // Hadoop YARN/2.0.x
  case "hadoopYarn":
    hadoopVersion = hadoopYarnVersion
    hadoopMsg = "Using Apache Hadoop YARN [$hadoopVersion]"
    hadoopClient = ["org.apache.hadoop:hadoop-client:$hadoopVersion"]

    break;
  // Hadoop 1.2.x
  case "hadoop12":
    hadoopVersion = hadoop12Version
    hadoopMsg =  "Using Apache Hadoop 1.2.x [$hadoopVersion]"
    hadoopClient = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"]

    break;
	
  // Hadoop 1.1.x
  case "hadoop11":
    hadoopVersion = hadoop11Version
    hadoopMsg = "Using Apache Hadoop 1.1.x [$hadoopVersion]"
    hadoopClient = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"]

    break;

  // Hadoop 1.0.x
  case "hadoop10":
    hadoopVersion = hadoop10Version
    hadoopMsg = "Using Apache Hadoop 1.0.x [$hadoopVersion]"
    hadoopClient = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"]

    break;

  default:
    hadoopMsg = "Using Apache Hadoop Stable [$hadoopVersion]"
    hadoopVersion = hadoopStableVersion
    hadoopClient = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"] 
}
allprojects {
	dependencies {
		
		testCompile "junit:junit:$junitVersion"
		testCompile "org.hamcrest:hamcrest-all:$hamcrestVersion"
		testCompile "org.elasticsearch:elasticsearch:$esVersion"
	    testRuntime "log4j:log4j:$log4jVersion"
	}

if (rootProject == project) {
	println hadoopMsg

	repositories {
	  // JDO ec2 missing from Maven Central
	  maven { url "http://www.datanucleus.org/downloads/maven2" }
	  maven { url "http://conjars.org/repo" }
	}

	dependencies {
		provided(hadoopClient)
		provided("org.codehaus.jackson:jackson-mapper-asl:$jacksonVersion")

		// Pig
		optional("org.apache.pig:pig:$pigVersion")

		ext.hiveGroup = "org.apache.hive"
		// Hive
		optional("$hiveGroup:hive-service:$hiveVersion")
		
		// Cascading
		optional("cascading:cascading-hadoop:$cascadingVersion")
		optional("cascading:cascading-local:$cascadingVersion")
	
		// Testing
		if (hadoopVersion.contains("1.0.")) {
			// missing dependency in Hadoop 1.0.3/1.0.4
			testCompile "commons-io:commons-io:2.1"
		}
		
		// testRuntime "$hiveGroup:hive-builtins:$hiveVersion"

		// Required by Pig
		testRuntime "com.google.guava:guava:11.0"
		testRuntime "dk.brics.automaton:automaton:1.11-8"
		optional("joda-time:joda-time:$jodaVersion")
		
		// Required by Hive + Pig
		// testRuntime "org.antlr:antlr-runtime:$antlrVersion"
		
		// Required by Hive
		testRuntime "org.apache.hive:hive-jdbc:$hiveVersion"
	}
}
}

configurations.all {
  resolutionStrategy {
	forcedModules = ['commons-httpclient:commons-httpclient:3.0.1']
  }
}

configure(allprojects) { project ->
	
	compileJava {
		sourceCompatibility = 1.6
		targetCompatibility = 1.6 

		options.compilerArgs << "-Xlint:unchecked"
		//options.compilerArgs << "-Xlint:deprecation"
		options.compilerArgs << "-Xlint:options"
	}
	
	compileTestJava {
			sourceCompatibility = 1.6
			targetCompatibility = 1.6
	}
	
	test {
		jacoco {
			def jacocoEnabled = project.hasProperty("jacocoCoverage")
			enabled = jacocoEnabled
		   
			if (jacocoEnabled)
				println "Enabling Jacoco Coverage"
		}
	}
	
	task sourcesJar(type: Jar, dependsOn:classes) {
		classifier = 'sources'
		from sourceSets.main.allJava
	}

	artifacts {
		archives sourcesJar
	}
}

// enable javadoc only for root project
task javadocJar(type: Jar) {
	classifier = 'javadoc'
	from javadoc
}
	
artifacts {
	archives javadocJar
}

	
ext.skipCascading = true
ext.skipMR = true
ext.skipHive = true
ext.skipPig = true

task enableMRTests {
    description = "Enable Map/Reduce tests"
    group = "Verification"
    doLast() {
        project.ext.skipMR = false
   }
}

task enableCascadingTests {
    description = "Enable Cascading tests"
    group = "Verification"
    doLast() {
        project.ext.skipCascading = false
   }
}

task enableHiveTests {
    description = "Enable Hive tests"
    group = "Verification"
    doLast() {
        project.ext.skipHive = false
  }
}

task enablePigTests {
    description = "Enable Pig tests"
    group = "Verification"
    
    doLast() {
        project.ext.skipPig = false
  }
}

task enableIntegrationTests() {
    description = "Enable integration tests"
    group = "Verification"
    doLast() {
      println "Enable all integration tests"
	  enableMRTests.execute()
  	  enableCascadingTests.execute()
	  enablePigTests.execute()
	  //enableHiveTests.execute()
    }
}

test {
    systemProperties['input.path'] = 'build/classes/test/input'
    systemProperties['output.path'] = 'build/classes/test/output'
    includes = ["org/elasticsearch/hadoop/serialization/*.class", 
				"org/elasticsearch/hadoop/pig/*.class",
				"org/elasticsearch/hadoop/rest/*.class",
				"org/elasticsearch/hadoop/util/**/*.class", 
				"org/elasticsearch/hadoop/integration/**/*Suite.class"]

	minHeapSize = "256m"
	maxHeapSize = "768m"

    testLogging {
        events "started" //, "standardOut", "standardError"
        minGranularity 2
        maxGranularity 2
    }
	
    doFirst() {
        ext.msg = ""

        if (skipMR) {
            ext.msg += "MapReduce "
            excludes.add("**/integration/mr/**")
        }
        if (skipCascading) {
            ext.msg += "Cascading "
            excludes.add("**/integration/cascading/**")
        }
        if (skipHive) {
            ext.msg += "Hive "
            excludes.add("**/integration/hive/**")
        }
        if (skipPig) {
            ext.msg += "Pig"
            excludes.add("**/integration/pig/**")
        }

        if (!msg.isEmpty())
            println "Skipping [$msg] Tests";
	}
}

javadoc {
	ext.srcDir = file("${rootProject}/docs/src/api")
	

	configure(options) {
		//stylesheetFile = file("${rootProject.projectDir}/docs/src/api/javadoc.css")
		//overview = "${rootProject.projectDir}/docs/src/api/overview.html"
		docFilesSubDirs = true
		outputLevel = org.gradle.external.javadoc.JavadocOutputLevel.QUIET
		breakIterator = true
		author = true
		showFromProtected()

		groups = [
		'Elasticsearch Map/Reduce' : ['org.elasticsearch.hadoop.mr*'],
		'Elasticsearch Cascading' : ['org.elasticsearch.hadoop.cascading*'],
		'Elasticsearch Hive' : ['org.elasticsearch.hadoop.hive*'],
		'Elasticsearch Pig' : ['org.elasticsearch.hadoop.pig*'],
		]

		links = [
			"http://download.oracle.com/javase/6/docs/api",
			"http://commons.apache.org/proper/commons-logging/apidocs/",
			"http://hadoop.apache.org/common/docs/stable1/api",
			//"http://hbase.apache.org/apidocs/",
			"http://pig.apache.org/docs/r0.12.0/api/",
			"http://hive.apache.org/javadocs/r0.12.0/api/",
			"http://docs.cascading.org/cascading/2.1/javadoc/"
		]

		excludes = [
			"org/elasticsearch/hadoop/rest/**", 
			"org/elasticsearch/hadoop/serialization/**", 
			"org/elasticsearch/hadoop/util/**"
		]
	}

	title = "${rootProject.description} ${version} API"
}

// jar used for testing Hadoop remotely (es-hadoop + tests)
task hadoopTestingJar(type: Jar) {
	classifier = 'testing'
	from sourceSets.test.output
	from sourceSets.main.output
}

configure(allprojects) { project ->

	licenseMain {
		header rootProject.file('src/test/resources/HEADER')
		strictCheck false
		ignoreFailures true
	}
	licenseTest.enabled = false
	// exclude properties file
	tasks.withType(nl.javadude.gradle.plugins.license.License).each { licenseTask ->
		licenseTask.exclude '**/*.properties'
		licenseTask.exclude '**/package-info.java'
	}
}

jar {
	manifest.attributes["Created-By"] = "${System.getProperty("java.version")} (${System.getProperty("java.specification.vendor")})"
	manifest.attributes['Implementation-Title'] = 'elasticsearch-hadoop'
	manifest.attributes['Implementation-Version'] = project.version
	manifest.attributes['Implementation-URL'] = "http://github.com/elasticsearch/elasticsearch-hadoop"
	manifest.attributes['Implementation-Vendor'] = "Elasticsearch"
	manifest.attributes['Implementation-Vendor-Id'] = "org.elasticsearch.hadoop"
	
	def build = System.env['ESHDP.BUILD']
	if (build != null)
		manifest.attributes['Build'] = build
	
	String rev = "unknown"
	
	// parse the git files to find out the revision
	File gitHead = file('.git/HEAD')
	if (gitHead.exists()) {
		gitHead = file('.git/' + gitHead.text.trim().replace('ref: ',''))
		if (gitHead.exists()) { rev = gitHead.text }
	}

	from("$rootDir/docs/src/info") {
		include "license.txt"
		include "notice.txt"
		into "META-INF"
		expand(copyright: new Date().format('yyyy'), version: project.version)
	}

	manifest.attributes['Repository-Revision'] = rev
}

// this is in just as a convenience - the official docs are generated through an external process
asciidoctor {
	//logDocuments = true
	sourceDir = file("docs/src/reference/asciidoc")
	sourceDocumentName = file("docs/src/reference/asciidoc/index.adoc")
    options = [
		doctype: 'book',
        attributes: [
			'source-highlighter': 'highlightjs',
			icons: 'font',
			badges: '',
			toc2: '',
			sectanchors: '',
			//stylesheet: 'github.css',
			copycss: '',
			revnumber : project.version
			//'linkcss!': ''
        ]
    ]
}

// reporting
configure(allprojects) { project ->
	findbugsMain {
	  ignoreFailures = true
	  effort = "max"
	  reportLevel = "low"
	  
	  reports {
		xml.enabled = true
		html.enabled = false
	  }
	}

	pmdMain {
	  ignoreFailures = true
	  reports {
		xml.enabled = true
		html.enabled = true
	  }
	}

	jacoco {
		toolVersion = "0.6.3.201306030806"
	}

	jacocoTestReport {
		reports {
			xml.enabled true
			csv.enabled true
			html.enabled true
		}
	}

	findbugsTest.enabled = false
	pmdTest.enabled = false   
}

// packaging
task distZip(type: Zip, dependsOn: [jar, sourcesJar, javadocJar]) {
    group = "Distribution"
    description = "Builds -${classifier} archive, containing all jars and docs, suitable for download page."
	ext.folderName = "${baseName}" + "-" + "${version}"
	
    from(".") {
        include "README.md"
        include "LICENSE.txt"
        include "NOTICE.txt"
        into folderName
        //expand(yyyy: new Date().format("yyyy"), version: project.version)
    }

    from("build/asciidoc") {
        into "${folderName}/docs"
    }

    into ("${folderName}/dist") {
        from "build/libs"
		exclude "*-testing.jar"
    }
}

task wrapper(type: Wrapper) {
    description = 'Generates gradlew[.bat] scripts'
    gradleVersion = '1.8'
}

// skip creation of javadoc jars 
assemble.dependsOn = ['jar', 'hadoopTestingJar']
defaultTasks 'build'