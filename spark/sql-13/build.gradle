
description = "Elasticsearch Spark (for Spark 1.3-1.6)"

evaluationDependsOn(':elasticsearch-hadoop-mr')

apply plugin: 'es.hadoop.build'
apply plugin: 'scala'
apply plugin: 'scala.variants'

variants {
    defaultVersion '2.11.12'
    targetVersions '2.10.7', '2.11.12'
}

println "Compiled using Scala ${project.ext.scalaMajorVersion} [${project.ext.scalaVersion}]"
String sparkVersion = spark13Version

compileScala {
    configure(scalaCompileOptions.forkOptions) {
        memoryMaximumSize = '1g'
        jvmArgs = ['-XX:MaxPermSize=512m']
    }
    scalaCompileOptions.additionalParameters = [
            "-feature",
            "-unchecked",
            "-deprecation",
            "-Xfuture",
            "-Yno-adapted-args",
            "-Ywarn-dead-code",
            "-Ywarn-numeric-widen",
            "-Xfatal-warnings"
    ]
}

String coreSrc = file("$projectDir/../core").absolutePath.replace('\\','/')

sourceSets {
    main.scala.srcDirs += "$coreSrc/main/scala"
    test.scala.srcDirs += "$coreSrc/test/scala"
    itest.java.srcDirs += "$coreSrc/itest/java"
    itest.scala.srcDirs += "$coreSrc/itest/scala"
    itest.resources.srcDirs += "$coreSrc/itest/resources"
}


// currently the outside project folders are transformed into linked resources however
// Gradle only supports one so the project will be invalid as not all sources will be in there
// as such, they are setup here manually for Eclipse. IntelliJ probably needs a similar approach
eclipse {
    project.file.whenMerged { pj ->
        // eliminated resources created by gradle

        linkedResources.clear()
        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/main/scala", "2", "$coreSrc/main/scala", null))
        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/test/scala", "2", "$coreSrc/test/scala", null))
        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/itest/java", "2", "$coreSrc/itest/java", null))
        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/itest/scala", "2", "$coreSrc/itest/scala", null))
        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/itest/resources","2", "$coreSrc/itest/resources", null))

    }
    classpath.file {
        whenMerged { cp ->
            entries.removeAll { entry ->
                entry.kind == 'src' && (entry.path in ["scala", "java", "resources"] || entry.path.startsWith("itest-") || entry.path.endsWith("-scala"))
            }

            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/main/scala", null))
            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/test/scala", null))
            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/itest/java", null))
            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/itest/scala", null))
            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/itest/resources", null))
        }
    }
}

dependencies {
    provided(project(":elasticsearch-hadoop-mr"))
    provided(project(path: ":elasticsearch-hadoop-mr", configuration:"compile"))

    compile("org.scala-lang:scala-library:${project.ext.scalaVersion}")

    provided("org.apache.spark:spark-core_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'javax.servlet'
        exclude group: 'org.apache.hadoop'
    }

    optional("org.apache.spark:spark-sql_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'org.apache.hadoop'
    }
    optional("org.apache.spark:spark-streaming_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'org.apache.hadoop'
    }

    testCompile project(":elasticsearch-hadoop-mr").sourceSets.test.runtimeClasspath

    itestCompile project(":elasticsearch-hadoop-mr").sourceSets.itest.runtimeClasspath
    itestCompile("org.apache.spark:spark-streaming_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'org.apache.hadoop'
    }
    itestCompile("org.apache.spark:spark-sql_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'org.apache.hadoop'
    }
}

jar {
    from(zipTree(project(":elasticsearch-hadoop-mr").jar.archivePath)) {
        include "org/elasticsearch/hadoop/**"
        include "esh-build.properties"
    }
}

javadoc {
    source += project(":elasticsearch-hadoop-mr").sourceSets.main.allJava
    classpath += files(project(":elasticsearch-hadoop-mr").sourceSets.main.compileClasspath)
}

sourcesJar {
    from project(":elasticsearch-hadoop-mr").sourceSets.main.allJava.srcDirs
}

scaladoc {
    title = "${rootProject.description} ${version} API"
}

// deal with the messy conflicts out there
configurations.all { Configuration conf ->
    conf.resolutionStrategy {
        eachDependency { details ->
            // in a similar vein, change all javax.servlet artifacts to the one used by Spark
            // otherwise these will lead to SecurityException (signer information wrong)
            if (details.requested.name.contains("servlet") && !details.requested.name.contains("guice")) {
                details.useTarget group: "org.eclipse.jetty.orbit", name: "javax.servlet", version: "3.0.0.v201112011016"
            }
        }
    }

    conf.exclude group: "org.mortbay.jetty"
}