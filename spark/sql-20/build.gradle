
description = "Elasticsearch Spark (for Spark 2.X)"

evaluationDependsOn(':elasticsearch-hadoop-mr')

apply plugin: 'scala'
apply plugin: 'es.hadoop.build.integration'
apply plugin: 'scala.variants'
apply plugin: 'ivy-publish'
apply plugin: 'maven-publish'

variants {
    defaultVersion '2.12.10'
    targetVersions '2.10.7', '2.11.12', '2.12.10'
}

configurations {
    scalaCompilerPlugin {
        defaultDependencies { dependencies ->
            dependencies.add(project.dependencies.create( "com.typesafe.genjavadoc:genjavadoc-plugin_${scalaVersion}:0.15"))
        }
    }
}

println "Compiled using Scala ${project.ext.scalaMajorVersion} [${project.ext.scalaVersion}]"
String sparkVersion = spark20Version

// Revert to spark 2.2.0 for scala 2.10 as 2.3+ does not support scala 2.10
if (project.ext.scalaMajorVersion == '2.10') {
    sparkVersion = '2.2.0'
}

tasks.withType(ScalaCompile) { ScalaCompile task ->
    task.sourceCompatibility = project.ext.minimumRuntimeVersion
    task.targetCompatibility = project.ext.minimumRuntimeVersion
}

compileScala {
    configure(scalaCompileOptions.forkOptions) {
        memoryMaximumSize = '1g'
        jvmArgs = ['-XX:MaxPermSize=512m']
    }
    scalaCompileOptions.additionalParameters = [
        "-feature",
        "-unchecked",
        "-deprecation",
        "-Xfuture",
        "-Yno-adapted-args",
        "-Ywarn-dead-code",
        "-Ywarn-numeric-widen",
        "-Xfatal-warnings"
    ]
}

String coreSrc = file("$projectDir/../core").absolutePath.replace('\\','/')

sourceSets {
    main.scala.srcDirs += "$coreSrc/main/scala"
    test.scala.srcDirs += "$coreSrc/test/scala"
    itest.java.srcDirs += "$coreSrc/itest/java"
    itest.scala.srcDirs += "$coreSrc/itest/scala"
    itest.resources.srcDirs += "$coreSrc/itest/resources"
}


// currently the outside project folders are transformed into linked resources however
// Gradle only supports one so the project will be invalid as not all sources will be in there
// as such, they are setup here manually for Eclipse. IntelliJ probably needs a similar approach
eclipse {
    project.file.whenMerged { pj ->
        // eliminated resources created by gradle

        linkedResources.clear()
        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/main/scala", "2", "$coreSrc/main/scala", null))
        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/test/scala", "2", "$coreSrc/test/scala", null))
        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/itest/java", "2", "$coreSrc/itest/java", null))
        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/itest/scala", "2", "$coreSrc/itest/scala", null))
        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/itest/resources","2", "$coreSrc/itest/resources", null))

    }
    classpath.file {
        whenMerged { cp ->
            entries.removeAll { entry ->
                entry.kind == 'src' && (entry.path in ["scala", "java", "resources"] || entry.path.startsWith("itest-") || entry.path.endsWith("-scala"))
            }

            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/main/scala", null))
            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/test/scala", null))
            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/itest/java", null))
            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/itest/scala", null))
            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/itest/resources", null))
        }
    }
}

dependencies {
    provided(project(":elasticsearch-hadoop-mr"))
    provided(project(path: ":elasticsearch-hadoop-mr", configuration:"compile"))

    compile("org.scala-lang:scala-library:$scalaVersion")
    compile("org.scala-lang:scala-reflect:$scalaVersion")

    provided("org.apache.spark:spark-core_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'javax.servlet'
        exclude group: 'org.apache.hadoop'
    }
    compileOnly("com.fasterxml.jackson.module:jackson-module-scala_${project.ext.scalaMajorVersion}:2.6.7.1")
    compileOnly("com.fasterxml.jackson.core:jackson-annotations:2.6.7")
    compileOnly("org.json4s:json4s-jackson_${project.ext.scalaMajorVersion}:3.2.11")
    compileOnly("org.slf4j:slf4j-api:1.7.6")

    if ('2.10'.equals(scalaMajorVersion)) {
        compileOnly("org.apache.spark:spark-unsafe_${project.ext.scalaMajorVersion}:$sparkVersion")
        compileOnly("org.apache.avro:avro:1.7.7")
        compileOnly("log4j:log4j:1.2.17")
        compileOnly("com.google.code.findbugs:jsr305:2.0.1")
        compileOnly("org.json4s:json4s-ast_2.10:3.2.10")
        compileOnly("com.esotericsoftware.kryo:kryo:2.21")
    }
    
    optional("org.apache.spark:spark-yarn_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'org.apache.hadoop'
    }
    optional("org.apache.spark:spark-sql_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'org.apache.hadoop'
    }
    compileOnly("org.apache.spark:spark-catalyst_${project.ext.scalaMajorVersion}:$sparkVersion")
    compileOnly("org.apache.spark:spark-tags_${project.ext.scalaMajorVersion}:$sparkVersion")
    compileOnly("com.google.protobuf:protobuf-java:2.5.0")

    optional("org.apache.spark:spark-streaming_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'org.apache.hadoop'
    }

    testCompile "org.elasticsearch:securemock:1.2"
    testCompile(project(":elasticsearch-hadoop-mr").sourceSets.test.output)

    itestCompile(project(":elasticsearch-hadoop-mr").sourceSets.itest.output)

    testCompile(project.ext.hadoopClient)
    testCompile("org.apache.spark:spark-core_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'javax.servlet'
        exclude group: 'org.apache.hadoop'
    }
    itestCompile("org.apache.spark:spark-yarn_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'org.apache.hadoop'
    }
    testCompile("org.apache.spark:spark-sql_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'org.apache.hadoop'
    }
    itestCompile("org.apache.spark:spark-streaming_${project.ext.scalaMajorVersion}:$sparkVersion") {
        exclude group: 'org.apache.hadoop'
    }
}

jar {
    from(zipTree(project(":elasticsearch-hadoop-mr").jar.archivePath)) {
        include "org/elasticsearch/hadoop/**"
        include "esh-build.properties"
        include "META-INF/services/*"
    }
}

javadoc {
    dependsOn compileScala
    source += project(":elasticsearch-hadoop-mr").sourceSets.main.allJava
    source += "$buildDir/generated/java"
    classpath += files(project(":elasticsearch-hadoop-mr").sourceSets.main.compileClasspath)
}

sourcesJar {
    from project(":elasticsearch-hadoop-mr").sourceSets.main.allJava.srcDirs
}

scaladoc {
    title = "${rootProject.description} ${version} API"
}

tasks.withType(ScalaCompile) {
    scalaCompileOptions.with {
        additionalParameters = [
                "-Xplugin:" + configurations.scalaCompilerPlugin.asPath,
                "-P:genjavadoc:out=$buildDir/generated/java".toString()
        ]
    }
}