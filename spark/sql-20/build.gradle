import org.elasticsearch.hadoop.gradle.scala.SparkVariantPlugin

description = "Elasticsearch Spark (for Spark 2.X)"

apply plugin: 'java-library'
apply plugin: 'scala'
apply plugin: 'es.hadoop.build.integration'
apply plugin: 'spark.variants'

//// A rule declaring consumer configurations that require the LibraryElements attribute to
//// equal "testing-classes" are allowed to use "jar" LibraryElement attributes as well.
//class TestingClassesRule implements AttributeCompatibilityRule<LibraryElements> {
//    def validConsumers = ['testing-classes', 'integration-testing-classes']
//    @Override
//    void execute(CompatibilityCheckDetails<LibraryElements> details) {
//        if (validConsumers.contains(details.consumerValue.name) && details.producerValue.name == 'jar') {
//            details.compatible()
//        }
//    }
//}
//
//// Install the compatibility rule
//dependencies {
//    attributesSchema {
//        attribute(LibraryElements.LIBRARY_ELEMENTS_ATTRIBUTE) {
//            compatibilityRules.add(TestingClassesRule)
//        }
//    }
//}

sparkVariants {
    capabilityGroup 'org.elasticsearch.spark.sql.variant'
    setDefaultVariant "spark20scala211", spark24Version, scala211Version
    addFeatureVariant "spark20scala210", spark22Version, scala210Version

    all { SparkVariantPlugin.SparkVariant variant ->
        String scalaCompileTaskName = project.sourceSets
                .getByName(variant.getSourceSetName("main"))
                .getCompileTaskName("scala")

        project.configurations {
            create(variant.configuration('embedded')) {
                transitive = false
                canBeResolved = true
            }
            getByName(variant.configuration('implementation')) {
                extendsFrom project.configurations.getByName(variant.configuration('embedded'))
            }
//            // test runtime should require testing classes
//            getByName(variant.configuration('test', 'runtimeClasspath')) {
//                attributes {
//                    attribute(LibraryElements.LIBRARY_ELEMENTS_ATTRIBUTE, objects.named(LibraryElements, 'testing-classes'))
//                }
//            }
//            // itest runtime requires spark core testing classes
//            getByName(variant.configuration('itest', 'runtimeClasspath')) {
//                attributes {
//                    attribute(LibraryElements.LIBRARY_ELEMENTS_ATTRIBUTE, objects.named(LibraryElements, 'integration-testing-classes'))
//                }
//            }
        }

        // Configure main compile task
        project.getTasks().getByName(scalaCompileTaskName) { ScalaCompile compileScala ->
            configure(compileScala.scalaCompileOptions.forkOptions) {
                memoryMaximumSize = '1g'
                jvmArgs = ['-XX:MaxPermSize=512m']
            }
            compileScala.scalaCompileOptions.additionalParameters = [
                    "-feature",
                    "-unchecked",
                    "-deprecation",
                    "-Xfuture",
                    "-Yno-adapted-args",
                    "-Ywarn-dead-code",
                    "-Ywarn-numeric-widen",
                    "-Xfatal-warnings"
            ]
        }

        dependencies {
            add(variant.configuration('embedded'), project(":elasticsearch-hadoop-mr"))
//            add(variant.configuration('embedded'), project(":elasticsearch-spark"))
            add(variant.configuration('embedded'), project(":elasticsearch-spark")) {
                capabilities {
                    requireCapability("org.elasticsearch.spark.variant:$variant.name:$project.version")
                }
            }

//            testCompile(project(":elasticsearch-spark").sourceSets.test.output)
//            add(variant.configuration('itest', 'implementation'), project(":elasticsearch-spark")) {
//                capabilities {
//                    requireCapability("org.elasticsearch.spark.variant:$variant.name:$project.version")
//                }
//            }

            add(variant.configuration('api'), "org.scala-lang:scala-library:$variant.scalaVersion")
            add(variant.configuration('api'), "org.scala-lang:scala-reflect:$variant.scalaVersion")
            add(variant.configuration('api'), "org.apache.spark:spark-core_${variant.scalaMajorVersion}:$variant.sparkVersion") {
                exclude group: 'javax.servlet'
                exclude group: 'org.apache.hadoop'
            }

            add(variant.configuration('implementation'), "org.apache.spark:spark-sql_${variant.scalaMajorVersion}:$variant.sparkVersion") {
                exclude group: 'org.apache.hadoop'
            }
            add(variant.configuration('implementation'), "org.apache.spark:spark-streaming_${variant.scalaMajorVersion}:$variant.sparkVersion") {
                exclude group: 'org.apache.hadoop'
            }
            add(variant.configuration('implementation'), "org.slf4j:slf4j-api:1.7.6") {
                because 'spark exposes slf4j components in traits that we need to extend'
            }
            add(variant.configuration('implementation'), "commons-logging:commons-logging:1.1.1")
            add(variant.configuration('implementation'), "javax.xml.bind:jaxb-api:2.3.1")
            add(variant.configuration('implementation'), "com.google.protobuf:protobuf-java:2.5.0")
            add(variant.configuration('implementation'), "org.apache.spark:spark-catalyst_${variant.scalaMajorVersion}:$variant.sparkVersion")
            add(variant.configuration('implementation'), "org.apache.spark:spark-yarn_${variant.scalaMajorVersion}:$variant.sparkVersion") {
                exclude group: 'org.apache.hadoop'
            }

            // Scala compiler needs these for arcane reasons, but they are not used in the api nor the runtime
            add(variant.configuration('compileOnly'), "com.fasterxml.jackson.core:jackson-annotations:2.6.7")
            add(variant.configuration('compileOnly'), "org.json4s:json4s-jackson_${variant.scalaMajorVersion}:3.2.11")
            add(variant.configuration('compileOnly'), "org.apache.spark:spark-tags_${variant.scalaMajorVersion}:$variant.sparkVersion")

            if ('2.10' == scalaMajorVersion) {
                add(variant.configuration('implementation'), "org.apache.spark:spark-unsafe_${variant.scalaMajorVersion}:$variant.sparkVersion")
                add(variant.configuration('implementation'), "org.apache.avro:avro:1.7.7")
                add(variant.configuration('implementation'), "log4j:log4j:1.2.17")
                add(variant.configuration('implementation'), "com.google.code.findbugs:jsr305:2.0.1")
                add(variant.configuration('implementation'), "org.json4s:json4s-ast_2.10:3.2.10")
                add(variant.configuration('implementation'), "com.esotericsoftware.kryo:kryo:2.21")
                add(variant.configuration('compileOnly'), "org.apache.hadoop:hadoop-annotations:${project.ext.hadoopVersion}")
                add(variant.configuration('compileOnly'), "org.codehaus.jackson:jackson-core-asl:${project.ext.jacksonVersion}")
                add(variant.configuration('compileOnly'), "org.codehaus.jackson:jackson-mapper-asl:${project.ext.jacksonVersion}")
            }

            add(variant.configuration('test', 'implementation'), project(":test:shared"))
//            add(variant.configuration('test', 'implementation'), project.ext.hadoopClient)
            add(variant.configuration('test', 'implementation'), "org.elasticsearch:securemock:1.2")
            add(variant.configuration('test', 'implementation'), "org.apache.spark:spark-core_${variant.scalaMajorVersion}:$variant.sparkVersion") {
                exclude group: 'javax.servlet'
                exclude group: 'org.apache.hadoop'
            }
            add(variant.configuration('test', 'implementation'), "org.apache.spark:spark-sql_${variant.scalaMajorVersion}:$variant.sparkVersion") {
                exclude group: 'org.apache.hadoop'
            }

            add(variant.configuration('itest', 'implementation'), project(":test:shared"))
            add(variant.configuration('itest', 'implementation'), "org.apache.spark:spark-yarn_${variant.scalaMajorVersion}:$variant.sparkVersion") {
                exclude group: 'org.apache.hadoop'
            }
            add(variant.configuration('itest', 'implementation'), "org.apache.spark:spark-streaming_${variant.scalaMajorVersion}:$variant.sparkVersion") {
                exclude group: 'org.apache.hadoop'
            }

            add(variant.configuration('additionalSources'), project(":elasticsearch-hadoop-mr"))
            add(variant.configuration('javadocSources'), project(":elasticsearch-hadoop-mr"))

            add(variant.configuration('additionalSources'), project(":elasticsearch-spark")) {
                capabilities {
                    requireCapability("org.elasticsearch.spark.variant:$variant.name:$project.version")
                }
            }
            add(variant.configuration('javadocSources'), project(":elasticsearch-spark")) {
                capabilities {
                    requireCapability("org.elasticsearch.spark.variant:$variant.name:$project.version")
                }
            }
        }

        def javaFilesOnly = { FileTreeElement spec ->
            spec.file.name.endsWith('.java') || spec.isDirectory()
        }

        // Add java files from scala source set to javadocSourceElements.
        project.fileTree("src/main/scala").include(javaFilesOnly).each {
            project.artifacts.add(variant.configuration('javadocSourceElements'), it)
        }

        if (variant.scalaMajorVersion != '2.10') {
            // Configure java source generation for javadoc purposes
            String generatedJavaDirectory = "$buildDir/generated/java/${variant.name}"
            Configuration scalaCompilerPlugin = project.configurations.maybeCreate(variant.configuration('scalaCompilerPlugin'))
            scalaCompilerPlugin.defaultDependencies { dependencies ->
                dependencies.add(project.dependencies.create("com.typesafe.genjavadoc:genjavadoc-plugin_${variant.scalaVersion}:0.13"))
            }

            ScalaCompile compileScala = tasks.getByName(scalaCompileTaskName) as ScalaCompile
            compileScala.scalaCompileOptions.with {
                additionalParameters = [
                        "-Xplugin:" + configurations.getByName(variant.configuration('scalaCompilerPlugin')).asPath,
                        "-P:genjavadoc:out=$generatedJavaDirectory".toString()
                ]
            }
            // Export generated Java code from the genjavadoc compiler plugin
            artifacts {
                add(variant.configuration('javadocSourceElements'), project.file(generatedJavaDirectory)) {
                    builtBy compileScala
                }
            }
            tasks.getByName(variant.taskName('javadoc')) {
                dependsOn compileScala
                source(generatedJavaDirectory)
            }
        }

        scaladoc {
            title = "${rootProject.description} ${version} API"
        }
    }
}

// deal with the messy conflicts out there
// Ignore the scalaCompilerPlugin configurations since it is immediately resolved to configure the scala compiler tasks
configurations.matching{ it.name.contains('CompilerPlugin') == false }.all { Configuration conf ->
    conf.resolutionStrategy {
        eachDependency { details ->
            // change all javax.servlet artifacts to the one used by Spark otherwise these will lead to
            // SecurityException (signer information wrong)
            if (details.requested.name.contains("servlet") && !details.requested.name.contains("guice")) {
                details.useTarget group: "org.eclipse.jetty.orbit", name: "javax.servlet", version: "3.0.0.v201112011016"
            }
        }
    }
    conf.exclude group: "org.mortbay.jetty"
}

//variants {
//    defaultVersion '2.11.12'
//    targetVersions '2.10.7', '2.11.12'
//}
//
//configurations {
//    embedded {
//        transitive = false
//        canBeResolved = true
//    }
//    implementation {
//        extendsFrom project.configurations.embedded
//    }
//    if (project.ext.scalaMajorVersion != '2.10') {
//        scalaCompilerPlugin {
//            defaultDependencies { dependencies ->
//                dependencies.add(project.dependencies.create( "com.typesafe.genjavadoc:genjavadoc-plugin_${scalaVersion}:0.13"))
//            }
//        }
//    }
//}
//
//println "Compiled using Scala ${project.ext.scalaMajorVersion} [${project.ext.scalaVersion}]"
//String sparkVersion = spark20Version
//
//// Revert to spark 2.2.0 for scala 2.10 as 2.3+ does not support scala 2.10
//if (project.ext.scalaMajorVersion == '2.10') {
//    sparkVersion = '2.2.0'
//}

tasks.withType(ScalaCompile) { ScalaCompile task ->
    task.sourceCompatibility = project.ext.minimumRuntimeVersion
    task.targetCompatibility = project.ext.minimumRuntimeVersion
    task.options.forkOptions.executable = new File(project.ext.runtimeJavaHome, 'bin/java').absolutePath
}

//compileScala {
//    configure(scalaCompileOptions.forkOptions) {
//        memoryMaximumSize = '1g'
//        jvmArgs = ['-XX:MaxPermSize=512m']
//    }
//    scalaCompileOptions.additionalParameters = [
//        "-feature",
//        "-unchecked",
//        "-deprecation",
//        "-Xfuture",
//        "-Yno-adapted-args",
//        "-Ywarn-dead-code",
//        "-Ywarn-numeric-widen",
//        "-Xfatal-warnings"
//    ]
//}
//
//String coreSrc = file("$projectDir/../core").absolutePath.replace('\\','/')
//
//sourceSets {
//    main.scala.srcDirs += "$coreSrc/main/scala"
//    test.scala.srcDirs += "$coreSrc/test/scala"
//    itest.java.srcDirs += "$coreSrc/itest/java"
//    itest.scala.srcDirs += "$coreSrc/itest/scala"
//    itest.resources.srcDirs += "$coreSrc/itest/resources"
//}

//def javaFilesOnly = { FileTreeElement spec ->
//    spec.file.name.endsWith('.java') || spec.isDirectory()
//}
//
//artifacts {
//    sourceElements(project.file("$coreSrc/main/scala"))
//    // Add java files from core source to javadocElements.
//    project.fileTree("$coreSrc/main/scala").include(javaFilesOnly).each {
//        javadocElements(it)
//    }
//    project.fileTree("src/main/scala").include(javaFilesOnly).each {
//        javadocSourceElements(it)
//    }
//}

// currently the outside project folders are transformed into linked resources however
// Gradle only supports one so the project will be invalid as not all sources will be in there
// as such, they are setup here manually for Eclipse. IntelliJ probably needs a similar approach
//eclipse {
//    project.file.whenMerged { pj ->
//        // eliminated resources created by gradle
//
//        linkedResources.clear()
//        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/main/scala", "2", "$coreSrc/main/scala", null))
//        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/test/scala", "2", "$coreSrc/test/scala", null))
//        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/itest/java", "2", "$coreSrc/itest/java", null))
//        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/itest/scala", "2", "$coreSrc/itest/scala", null))
//        linkedResources.add(new org.gradle.plugins.ide.eclipse.model.Link("core/itest/resources","2", "$coreSrc/itest/resources", null))
//
//    }
//    classpath.file {
//        whenMerged { cp ->
//            entries.removeAll { entry ->
//                entry.kind == 'src' && (entry.path in ["scala", "java", "resources"] || entry.path.startsWith("itest-") || entry.path.endsWith("-scala"))
//            }
//
//            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/main/scala", null))
//            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/test/scala", null))
//            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/itest/java", null))
//            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/itest/scala", null))
//            entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder("core/itest/resources", null))
//        }
//    }
//}

//dependencies {
//    embedded(project(":elasticsearch-hadoop-mr"))
//
//    compile(project(":elasticsearch-spark"))
//    testCompile(project(":elasticsearch-spark").sourceSets.test.output)
//    itestCompile(project(":elasticsearch-spark").sourceSets.itest.output)
//
//    api("org.scala-lang:scala-library:$scalaVersion")
//    api("org.scala-lang:scala-reflect:$scalaVersion")
//    api("org.apache.spark:spark-core_${project.ext.scalaMajorVersion}:$sparkVersion") {
//        exclude group: 'javax.servlet'
//        exclude group: 'org.apache.hadoop'
//    }
//
//    implementation("org.apache.spark:spark-sql_${project.ext.scalaMajorVersion}:$sparkVersion") {
//        exclude group: 'org.apache.hadoop'
//    }
//    implementation("org.apache.spark:spark-streaming_${project.ext.scalaMajorVersion}:$sparkVersion") {
//        exclude group: 'org.apache.hadoop'
//    }
//    implementation("org.slf4j:slf4j-api:1.7.6") {
//        because 'spark exposes slf4j components in traits that we need to extend'
//    }
//    implementation("commons-logging:commons-logging:1.1.1")
//    implementation("javax.xml.bind:jaxb-api:2.3.1")
//    implementation("org.apache.spark:spark-catalyst_${project.ext.scalaMajorVersion}:$sparkVersion")
//    implementation("org.apache.spark:spark-yarn_${project.ext.scalaMajorVersion}:$sparkVersion") {
//        exclude group: 'org.apache.hadoop'
//    }
//
//    // Scala compiler needs these for arcane reasons, but they are not used in the api nor the runtime
//    compileOnly("com.fasterxml.jackson.core:jackson-annotations:2.6.7")
//    compileOnly("org.json4s:json4s-jackson_${project.ext.scalaMajorVersion}:3.2.11")
//    compileOnly("org.apache.spark:spark-tags_${project.ext.scalaMajorVersion}:$sparkVersion")
//
//    if ('2.10' == scalaMajorVersion) {
//        implementation("org.apache.spark:spark-unsafe_${project.ext.scalaMajorVersion}:$sparkVersion")
//        implementation("org.apache.avro:avro:1.7.7")
//        implementation("log4j:log4j:1.2.17")
//        implementation("com.google.code.findbugs:jsr305:2.0.1")
//        implementation("org.json4s:json4s-ast_2.10:3.2.10")
//        implementation("com.esotericsoftware.kryo:kryo:2.21")
//        compileOnly("org.apache.hadoop:hadoop-annotations:${project.ext.hadoopVersion}")
//        compileOnly("org.codehaus.jackson:jackson-core-asl:${project.ext.jacksonVersion}")
//        compileOnly("org.codehaus.jackson:jackson-mapper-asl:${project.ext.jacksonVersion}")
//    }
//
//    testImplementation(project(":test:shared"))
//    testImplementation(project.ext.hadoopClient)
//    testImplementation("org.elasticsearch:securemock:1.2")
//    testImplementation("org.apache.spark:spark-core_${project.ext.scalaMajorVersion}:$sparkVersion") {
//        exclude group: 'javax.servlet'
//        exclude group: 'org.apache.hadoop'
//    }
//    testImplementation("org.apache.spark:spark-sql_${project.ext.scalaMajorVersion}:$sparkVersion") {
//        exclude group: 'org.apache.hadoop'
//    }
//
//    itestImplementation(project(":test:shared"))
//    itestImplementation("org.apache.spark:spark-yarn_${project.ext.scalaMajorVersion}:$sparkVersion") {
//        exclude group: 'org.apache.hadoop'
//    }
//    itestImplementation("org.apache.spark:spark-streaming_${project.ext.scalaMajorVersion}:$sparkVersion") {
//        exclude group: 'org.apache.hadoop'
//    }
//
//    additionalSources(project(":elasticsearch-hadoop-mr"))
//    javadocSources(project(":elasticsearch-hadoop-mr"))
//}
//
//// Export generated Java code from the genjavadoc compiler plugin
//artifacts {
//    javadocSourceElements(project.file("$buildDir/generated/java")) {
//        builtBy compileScala
//    }
//}
//
sparkVariants {
    all { SparkVariantPlugin.SparkVariant variant ->
        tasks.getByName(variant.taskName('jar')) {
            dependsOn(project.configurations.getByName(variant.configuration('embedded')))
            from(project.configurations.getByName(variant.configuration('embedded')).collect { it.isDirectory() ? it : zipTree(it)}) {
                include "org/elasticsearch/**"
                include "esh-build.properties"
                include "META-INF/services/*"
            }
        }
    }
}
//jar {
//    dependsOn(project.configurations.embedded)
//    from(project.configurations.embedded.collect { it.isDirectory() ? it : zipTree(it)}) {
//        include "org/elasticsearch/hadoop/**"
//        include "esh-build.properties"
//        include "META-INF/services/*"
//    }
//
////    from(zipTree(project(":elasticsearch-spark").jar.archivePath)) {
////        include "org/elasticsearch/spark/**"
////        include "META-INF/services/*"
////    }
//}
//
//javadoc {
//    dependsOn compileScala
//    source += "$buildDir/generated/java"
//}
//
//scaladoc {
//    title = "${rootProject.description} ${version} API"
//}
//
//if (project.ext.scalaMajorVersion != '2.10') {
//    tasks.withType(ScalaCompile) {
//        scalaCompileOptions.with {
//            additionalParameters = [
//                    "-Xplugin:" + configurations.scalaCompilerPlugin.asPath,
//                    "-P:genjavadoc:out=$buildDir/generated/java".toString()
//            ]
//        }
//    }
//}
