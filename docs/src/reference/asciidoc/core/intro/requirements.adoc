[[requirements]]
== Requirements

Before running {eh}, please do check out the requirements below. This is even more so important when deploying {eh} across a cluster where the software on some machines might be slightly out of sync. While {eh} tries its best to fall back and do various validations of its environment, doing a quick sanity check especially during upgrades can save you a lot of headaches.

NOTE: make sure to verify *all* nodes in a cluster when checking the version of a certain artifact.

TIP: {eh} adds no extra requirements to Hadoop (or the various libraries built on top of it, such as Cascading or Pig) or {es} however as a rule of thumb, do use the latest stable version of the said library (checking the compatibility with Hadoop and the JDK, where applicable).

[[requirements-jdk]]
=== JDK

JDK level 6.0 (or above) just like Hadoop. As JDK 6 as well as JDK 7 have been both EOL-ed and are not supported by recent product updates, we strongly recommend using the latest JDK 8 (at least u20 or higher). If that is not an option, use JDK 7.0 update u55 (required for Elasticsearch 1.2 or higher). An up-to-date support matrix for Elasticsearch is available https://www.elastic.co/subscriptions/matrix[here]. Do note that the JVM versions are *critical* for a stable environment as an incorrect version can corrupt the data underneath as explained in this http://www.elastic.co/blog/java-1-7u55-safe-use-elasticsearch-lucene/[blog post].

One can check the available JDK version from the command line:

[source,bash]
----
$ java -version
java version "1.8.0_45"
----

[[requirements-es]]
=== {es}

version *0.90* or higher, though we *highly* recommend using the latest Elasticsearch (currently {es-v}) is needed to run {es}. Using a lower version is not possible as {eh} uses new features added in 0.90 for distributed, parallel interactions with {es}. We strongly recommend using the latest, stable version of Elasticsearch.

The {es} version is shown in its folder name:

["source","bash",subs="attributes"]
----
$ ls
elasticsearch-{es-v}
----

If {es} is running (locally or remotely), one can find out through REST its version:

["source","js",subs="attributes"]
----
$ curl -XGET http://localhost:9200
{
  "status" : 200,
  "name" : "Dazzler",
  "version" : {
    "number" : "{es-v}",
    ...
  },
  "tagline" : "You Know, for Search"
}
----

[[requirements-hadoop]]
=== Hadoop

Hadoop 1.x (ideally the latest stable version in the 1.x line, currently 1.2.1) or 2.x (ideally the latest stable version, currently 2.4.1). {eh} is tested daily against Apache Hadoop; any distro compatible with Apache Hadoop should work just fine.

To check the version of Hadoop, one can refer either to its folder or jars (which contain the version in their names) or from the command line:

[source, bash]
----
$ bin/hadoop version
Hadoop 1.2.1
----

As a guide, the table below lists the Hadoop-based distributions against with this version has been tested against at various points in time:

|===
| Distribution      | Release

| Apache Hadoop     | 2.6.x
| Apache Hadoop     | 2.4.x
| Apache Hadoop     | 2.2.x
| Apache Hadoop     | 1.2.x
| Apache Hadoop     | 1.1.x

| Amazon EMR        | 3.8.x
| Amazon EMR        | 3.7.x
| Amazon EMR        | 3.6.x
| Amazon EMR        | 3.5.x
| Amazon EMR        | 3.4.x
| Amazon EMR        | 3.3.x
| Amazon EMR        | 3.2.x
| Amazon EMR        | 3.1.x
| Amazon EMR        | 3.0.x
| Amazon EMR        | 2.4.x

| Cloudera CDH      | 5.4.x
| Cloudera CDH      | 5.3.x
| Cloudera CDH      | 5.2.x
| Cloudera CDH      | 5.1.x
| Cloudera CDH      | 5.0.x
| Cloudera CDH      | 4.5.x
| Cloudera CDH      | 4.4.x
| Cloudera CDH      | 4.2.x

| Hortonworks HDP   | 2.2.x
| Hortonworks HDP   | 2.1.x
| Hortonworks HDP   | 2.0.x
| Hortonworks HDP   | 1.3.x

| Pivotal HD        | 2.1.x
| Pivotal HD        | 2.0.x
| Pivotal HD        | 1.1.x

| MapR              | 4.1.x
| MapR              | 4.0.x
| MapR              | 3.1.x
| MapR              | 3.0.x
| MapR              | 2.1.x
|===

IMPORTANT: Use the table above for guidance only; if your distro (or its version) is not there, it does not mean {eh} is not compatible with it; rather go ahead and try it out and let us know how it went.

[[requirements-yarn]]
=== Apache YARN / Hadoop 2.x

{eh} binary can run transparently on both Hadoop 1.x and Yarn / Hadoop 2.x without any changes or modifications.

[[requirements-cascading]]
=== Cascading

Cascading version 2.1.x (2.1.6) or higher. We recommend using the latest release of Cascading (currently {cs-v}).

Since Cascading is a library, the best way to find out the target version is to look at its file name:

["source","bash",subs="attributes"]
----
$ ls
cascading-{cs-v}.jar
----

[[requirements-hive]]
=== Apache Hive

Apache Hive 0.10 or higher. We recommend using the latest release of Hive (currently {hv-v}).

One can find out the Hive version from its folder name or command-line:

["source","bash",subs="attributes"]
----
$ bin/hive --version
Hive version {hv-v}
----

[[requirements-pig]]
=== Apache Pig

Pig 0.10.0 or higher. We recommend using the latest release of Pig (currently {pg-v}).

In a similar fashion, Pig version can be discovered from its folder path or through the command-line:

["source","bash",subs="attributes"]
----
$ bin/pig -i
Apache Pig version {pg-v}
----

[[requirements-spark]]
=== Apache Spark

Spark 1.0.0 or higher. We recommend using the latest release of Spark (currently {sp-v}). As {eh} provides
native integration (which is recommended) with {sp} it does not matter what binary one is using.
The same applies when using the Hadoop layer to integrate the two as {eh} supports the majority of
Hadoop distributions out there.

The Spark version can be typically discovery by looking at its folder name:

["source","bash",subs="attributes"]
----
$ pwd
/libs/spark/spark-{sp-v}-bin-XXXXX
----

or by running its shell:

["source","bash",subs="attributes"]
----
$ bin/spark-shell
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version {sp-v}
      /_/
...
----

[[requirements-spark-sql]]
==== Apache Spark SQL

If planning on using Spark SQL make sure to download the appropriate jar. While it is part of the Spark distribution,
it is _not_ part of Spark core but rather has its own jar. Thus, when constructing the classpath make sure to
include +spark-sql-<scala-version>.jar+ or the Spark _assembly_ : +spark-assembly-{sp-v}-<distro>.jar+

{eh} supports Spark SQL {sp-v} through the main jar. Since Spark SQL 1.3 is _not_
https://spark.apache.org/docs/latest/sql-programming-guide.html#upgrading-from-spark-sql-10-12-to-13[backwards compatible]
with Spark SQL 1.2 or 1.1, {eh} provides a dedicated jar. See the Spark chapter for more information.

[[requirements-storm]]
=== Apache Storm

Storm 0.9.2 or higher. We recommend using the latest release of Storm (currently {st-v}).

One can discover the Storm version by looking at its folder or by invoking the command:

["source","bash",subs="attributes"]
----
$ bin/storm version
{st-v}
----

