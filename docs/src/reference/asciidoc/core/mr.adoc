[[mapreduce]]
== {mr} integration

For low-level or performance-sensitive environments, {eh} provides dedicated `InputFormat` and `OutputFormat` implementations that can read and write data to {es}. The two IO interfaces will automatically convert JSON documents to `Map` of `Writable` objects and vice-versa.

[float]
=== Installation

In order to use {eh}, the jar needs to be available to the job class path. At ~`150kB` and without any dependencies, the jar can be either bundled in the job archive, manually or through CLI http://hadoop.apache.org/docs/r1.2.1/commands_manual.html#Generic`Options[Generic Options] (if your jar implements the http://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/util/Tool.html[Tool]), be distributed through Hadoop's http://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html#DistributedCache[DistributedCache] or made available by provisioning the cluster manually.

.CLI example

[source,bash]
----
$ bin/hadoop jar myJar.jar -libjars elasticsearch-hadoop.jar
----

[[type-conversion-writable]]
[float]
=== Type conversion

IMPORTANT: If automatic index creation is used, please review <<auto-mapping-type-loss,this>> section for more information.

{eh} automatically converts Hadoop built-in `Writable` types to {es} {ref}/mapping-core-types.html[types] (and back) as shown in the table below:

.`Writable` Conversion Table

[cols="^,^",options="header"]
|===
| `Writable` | {es} type

| `null`            | `null`
| `NullWritable`    | `null`
| `Text`            | `string`
| `UTF8`            | `string`
| `ByteWritable`    | `byte`
| `IntWritable`     | `int`
| `VInt`            | `int`
| `LongWritable`    | `long`
| `VLongWritable`   | `long`
| `ByteWritable`    | `binary`
| `DoubleWritable`  | `double`
| `FloatWritable`   | `float`
| `BooleanWritable` | `boolean`
| `MD5Writable`     | `string`
| `ArrayWritable`   | `array`
| `AbstractMapWritable` | `map`

|===

[float]
=== Writing data to {es}

With {eh}, {mr} jobs can write data to {es} making it searchable through {ref}/glossary.html#index[indexes]. {eh} supports both (so-called)  http://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/mapred/package-use.html['old'] and http://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/mapreduce/package-use.html['new'] Hadoop APIs.

`ESOutputFormat` expects a `Map<Writable, Writable>` value that it will convert into a JSON document; the key is ignored.

[float]
==== 'Old' (`org.apache.hadoop.mapred`) API

To write data to ES, use `org.elasticsearch.hadoop.mr.ESOutputFormat` on your job along with the relevant configuration <<configuration,properties>>:

[source,java]
----
JobConf conf = new JobConf();
conf.setSpeculativeExecution(false);            // disable speculative execution when writing to ES
conf.set("es.resource", "radio/artists");       // index used for storing data
conf.setOutputFormat(ESOutputFormat.class);     // use dedicated output format
...
JobClient.runJob(conf);
----

[float]
==== 'New' (`org.apache.hadoop.mapreduce`) API

Using the 'new' is strikingly similar - in fact, the exact same class (`org.elasticsearch.hadoop.mr.ESOutputFormat`) is used:

[source,java]
----
Configuration conf = new Configuration();
conf.set("es.resource", "radio/artists");       // index used for storing data
// disable speculative execution when writing to ES
conf.setBoolean("mapred.map.tasks.speculative.execution", false);
conf.setBoolean("mapred.reduce.tasks.speculative.execution", false);
Job job = new Job(conf);
job.setOutputFormat(ESOutputFormat.class);      // use dedicated output format
...
job.waitForCompletion(true);
----


[float]
=== Reading data from {es}

In a similar fashion, to read data from {es}, one needs to use `org.elasticsearch.hadoop.mr.ESInputFormat` class.
While it can read an entire index, it is much more convenient to actually execute a query and then feed the results back to Hadoop.

`ESInputFormat` returns a `Map<Writable, Writable>` converted from the JSON documents returned by {es} and a null (to be ignored) key.

[float]
==== 'Old' (`org.apache.hadoop.mapred`) API

Following our example above on radio artists, to get a hold of all the artists that start with 'me', one could use the following snippet:

[source,java]
----
JobConf conf = new JobConf();
conf.set("es.resource", "radio/artists/_search?q=me*"); // replace this with the relevant query
conf.setInputFormat(ESInputFormat.class);               // use dedicated input format
...
JobClient.runJob(conf);
----

[float]
==== 'New' (`org.apache.hadoop.mapreduce`) API

As expected, the `mapreduce` API version is quite similar:
[source,java]
----
Configuration conf = new Configuration();
conf.set("es.resource", "radio/artists/_search?q=me*"); // replace this with the relevant query
Job job = new Job(conf);                                // use dedicated input format
job.setInputFormat(ESInputFormat.class);
...
job.waitForCompletion(true);
----

////

== Putting it all together

.TODO
add example

////