[[spark]]
== Apache Spark support

[quote, Spark website]
____
http://spark.apache.org[Apache Spark] is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala and Python, and an optimized engine that supports general execution graphs.
____
Spark provides fast iterative/functional-like capabilities over large data sets, typically by _caching_ data in memory. As opposed to the rest of the libraries mentioned in this documentation, Apache Spark is computing framework that is not tied to {mr} itself however it does integrate with Hadoop, mainly to HDFS.
{eh} allows {es} to be used in Spark in two ways: through the dedicated support available since 2.1 or through the {mr} bridge since 2.0

[[spark-installation]]
[float]
=== Installation

Just like other libraries, {eh} needs to be available in Spark's classpath. As Spark has multiple deployment modes, this can translate to the target classpath, whether it is on only one node (as is the case with the local mode - which will be used through-out the documentation) or per-node depending on the desired infrastructure.

[[spark-native]]
[float]
=== Native support

added[2.1]
n
{eh} provides _native_ integration between {es} and {sp}, in the form of a +RDD+ (Resilient Distributed Dataset) (or _Pair_ +RDD+ to be precise) that can read data from Elasticsearch. The +RDD+ is offered in two 'flavors': one for Scala (which returns the data as +Tuple2+ with Scala collections) and one for Java (which returns the data as +Tuple2+ containing `java.util` collections).

IMPORTANT: Whenever possible, consider using the _native_ integration as it offers the best performance and maximum flexibility.

[[spark-native-cfg]]
[float]
==== Configuration

To configure one, one can set the various properties described in the <<configuration>> chapter through the http://spark.apache.org/docs/1.0.1/programming-guide.html#initializing-spark[`SparkConf`] object:

[source,scala]
----
import org.apache.spark.SparkConf

val conf = new SparkConf().setAppName(appName).setMaster(master)
conf.set("es.index.auto.create", "true")
----

[source,java]
----
SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);
conf.set("es.index.auto.create", "true");
----

[float]
[[spark-write]]
==== Writing data to {es}

With {eh}, any +RDD+ can be saved to {es} as long as its content can be translated into documents. In practice this means the +RDD+ type needs to be a +Map+ (whether a Scala or a Java one), a http://docs.oracle.com/javase/tutorial/javabeans/[+JavaBean+] or a Scala http://docs.scala-lang.org/tutorials/tour/case-classes.html[case class]. When that is not the case, one can easily _transform_ the data
in Spark or plug-in their own customer <<configuration-serialization,+ValueWriter+>>.

.Scala

When using Scala, simply import the `org.elasticsearch.spark` package which, through the http://www.artima.com/weblogs/viewpost.jsp?thread=179766[__pimp my library__] pattern, enriches the  _any_ +RDD+ API with `saveToEs` methods:

[source,scala]
----
import org.apache.spark.SparkContext    <1>
import org.apache.spark.SparkContext._

import org.elasticsearch.spark._        <2>

...

val conf = ...
val sc = new SparkContext(conf)         <3>

val numbers = Map("one" -> 1, "two" -> 2, "three" -> 3)
val airports = Map("arrival" -> "Otopeni", "SFO" -> "San Fran")

sc.makeRDD<4>(Seq(numbers, airports)).saveToEs<5>("spark/docs")
----

<1> Spark Scala imports
<2> {eh} Scala imports
<3> start Spark through its Scala API
<4> `makeRDD` creates an ad-hoc `RDD` based on the collection specified; any other +RDD+ (in Java or Scala) can be passed in
<5> index the content (namely the two _documents_ (numbers and airports)) in {es} under `spark/docs`

NOTE: Scala users might be tempted to use +Seq+ and the +->+ notation for declaring _root_ objects (that is the JSON document) instead of using a +Map+. While similar, the first notation results in slightly different types that cannot be matched to a JSON document: +Seq+ is an order sequence (in other words a list) while +<-+ creates a +Tuple+ which is more or less an ordered, fixed number of elements. As such, a list of lists cannot be used as a document since it cannot be mapped to a JSON object; however it can be used freely within one. Hence why in the example above ++Map(k->v)++ was used instead of ++Seq(k->v)++

As an alternative to the _implicit_ import above, one can {eh} Spark support in Scala through +EsSpark+ in +org.elasticsearch.spark.rdd+ package which acts as an utility class allowing explicit method invocations. Additionally instead of ++Map++s (which are convenient but require one mapping per instance due to their difference in structure), use a __case class__ :

[source,scala]
----
import org.apache.spark.SparkContext             
import org.elasticsearch.spark.rdd.EsSpark                        <1>

// define a case class
case class Trip(departure: String, arrival: String)               <2>

val upcomingTrip = Trip("OTP", "SFO")
val lastWeekTrip = Trip("MUC", "OTP")

val +RDD+ = sc.makeRDD(Seq(upcomingTrip, lastWeekTrip))           <3>
EsSpark.saveToEs(rdd, "spark/docs")                               <4>
----

<1> +EsSpark+ import
<2> Define a case class named +Trip+
<3> Create an +RDD+ around the +Trip+ instances
<4> Index the +RDD+ explicitly through +EsSpark+

For cases where the id (or other metadata fields like +ttl+ or +timestamp+) of the document needs to be specified, one can do so by setting the appropriate <<cfg-mapping, mapping>> namely +es.mapping.id+. Following the previous example, to indicate to {es} to use the field +id+ as the document id, update the +RDD+ configuration (it is also possible to set the property on the +SparkConf+ though due to its global effect it is discouraged):

[source,scala]
----
EsSpark.saveToEs(rdd, "spark/docs", Map("es.mapping.id" -> "id"))
----

.Java

Java users have a dedicated class that provides a similar functionality to +EsSpark+, namely +JavaEsSpark+ in the +org.elasticsearch.hadoop.spark.rdd.api.java+ (a package similar to Spark's https://spark.apache.org/docs/1.0.1/api/java/index.html?org/apache/spark/api/java/package-summary.html[Java API]):

[source,java]
----
import org.apache.spark.api.java.JavaSparkContext;                              <1>
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.SparkConf;

import org.elasticsearch.spark.rdd.java.api.JavaEsSpark;                        <2>
...

SparkConf conf = ...
JavaSparkContext jsc = new JavaSparkContext(conf);                              <3>

Map<String, ?> numbers = ImmutableMap.of("one", 1, "two", 2);                   <4>
Map<String, ?> airports = ImmutableMap.of("OTP", "Otopeni", "SFO", "San Fran");

JavaRDD<Map<String, ?>> javaRDD = jsc.parallelize(ImmutableList.of(doc1, doc2));<5>
JavaEsSpark.saveToEs(javaRDD, "spark/docs");                                    <6>
----

<1> Spark Java imports
<2> {eh} Java imports
<3> start Spark through its Java API
<4> to simplify the example, use https://code.google.com/p/guava-libraries/[Guava](a dependency of Spark) +Immutable+* methods for simple +Map+, +List+ creation
<5> create a simple +RDD+ over the two collections; any other +RDD+ (in Java or Scala) can be passed in
<6> index the content (namely the two _documents_ (numbers and airports)) in {es} under `spark/docs`

The code can be further simplifies by using Java 5 _static_ imports. Additionally, the +Map+ (who's mapping is dynamic due to its _loose_ structure) can be replaced with a +JavaBean+:

[source,java]
----
public class TripBean implements Serializable {
   private String departure, arrival;
   
   public TripBean(String departure, String arrival) { 
       setDeparture(departure);
       setArrival(arrival);
   }
   
   public TripBean() {}
   
   public String getDeparture() { return departure; }
   public String getArrival() { return arrival; }
   public void setDeparture(String dep) { departure = dep; }
   public void setArrival(String arr) { arrival = arr; }
}
----

[source,java]
----
import static org.elasticsearch.spark.java.api.JavaEsSpark;                <1>
...

TripBean upcoming = new TripBean("OTP", "SFO");
TripBean lastWeek = new TripBean("MUC", "OTP");

JavaRDD<TripBean> javaRDD = jsc.parallelize(
                            ImmutableList.of(upcoming, lastWeek));        <2>
saveToEs(javaRDD, "spark/docs");                                          <3>
----

<1> statically import +JavaEsSpark+
<2> define an +RDD+ containing +TripBean+ instances (+TripBean+ is a +JavaBean+)
<3> call +saveToEs+ method without having to type +JavaEsSpark+ again


Setting the document id (or other metadata fields like +ttl+ or +timestamp+) is similar to its Scala counterpart though potentially bit more verbose depending on whether you are using the JDK classes or some other utilities (like Guava):

[source,java]
----
JavaEsSpark.saveToEs(javaRDD, "spark/docs", ImmutableMap.of("es.mapping.id", "id"));
----

[float]
[[spark-write-json]]
==== Writing existing JSON to {es}

For cases where the data in the `RDD` is already in JSON, {eh} allows direct indexing _without_ applying any transformation; the data is taken as is and sent directly to {es}. As such, in this case, {eh} expects either an +RDD+
containing +String+ or byte arrays (+byte[]+/+Array[Byte]+), assuming each entry represents a JSON document. If the +RDD+ does not have the proper signature, the +saveJsonToEs+ methods cannot be applied (in Scala they will not be available).

.Scala

[source,scala]
----
val json1 = "{\"reason\" : \"business\",\"airport\" : \"SFO\"}"      <1>
val json2 = "{\"participants\" : 5,\"airport\" : \"OTP\"}"

new SparkContext(conf).makeRDD(Seq(json1, json2))
                                   .saveJsonToEs("spark/json-trips") <2>
----

<1> example of an entry within the +RDD+ - the JSON is _written_ as is, without any transformation
<2> index the JSON data through the dedicated +saveJsonToEs+ method

.Java

[source,java]
----
String json1 = "{\"reason\" : \"business\",\"airport\" : \"SFO\"}";  <1>
String json2 = "{\"participants\" : 5,\"airport\" : \"OTP\"}";

JavaContextSpark jsc = ...
JavaRDD<String><2> stringRDD = jsc.parallelize(ImmutableList.of(json1, json2));
JavaEsSpark.saveJsonToEs(stringRDD, "spark/json-trips");             <3>
----

<1> example of an entry within the +RDD+ - the JSON is _written_ as is, without any transformation
<2> notice the +RDD<String>+ signature
<3> index the JSON data through the dedicated +saveJsonToEs+ method

[float]
[[spark-write-dyn]]
==== Writing to dynamic/multi-resources

For cases when the data being written to {es} needs to be indexed under different buckets (based on the data content) one can use the `es.resource.write` field which accepts pattern that are resolved from the document content, at runtime. Following the aforementioned <<cfg-multi-writes,media example>>, one could configure it as follows:

.Scala

[source,scala]
----
val game = Map("media_type"<1>->"game","title" -> "FF VI","year" -> "1994")
val book = Map("media_type" -> "book","title" -> "Harry Potter","year" -> "2010")
val cd = Map("media_type" -> "music","title" -> "Surfing With The Alien")

sc.makeRDD(Seq(game, book, cd)).saveToEs("my-collection/{media-type}")  <2>
----

<1> Document _key_ used for splitting the data. Any field can be declared (but make sure it is available in all documents)
<2> Save each object based on its resource pattern, in this example based on +media_type+

For each document/object about to be written, {eh} will extract the +media_type+ field and use its value to determine the target resource.

.Java

As expected, things in Java are strikingly similar:

[source,java]
----
Map<String, ?> game = 
  ImmutableMap.of("media_type", "game", "title", "FF VI", "year", "1994");
Map<String, ?> book = ...
Map<String, ?> cd = ...

JavaRDD<Map<String, ?>> javaRDD = 
                jsc.parallelize(ImmutableList.of(game, book, cd));
saveToEs(javaRDD, "my-collection/{media-type}");  <1>
----

<1> Save each object based on its resource pattern, +media_type+ in this example

[float]
[[spark-write-meta]]
==== Handling document metadata

{es} allows each document to have its own http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/\_document\_metadata.html[metadata]. As explained above, through the various <<cfg-mapping, mapping>> options one can customize these parameters so that their values are extracted from their belonging document. Further more, one can even include/exclude what parts of the data are backed to {es}. In Spark, {eh} extends this functionality allowing metadata to be supplied _outside_ the document itself through the use of http://spark.apache.org/docs/latest/programming-guide.html#working-with-key-value-pairs[_pair_ ++RDD++s].
In other words, for ++RDD++s containing a key-value tuple, the metadata can be extracted from the key and the value used as the document source.

The metadata is described through the +Metadata+ Java http://docs.oracle.com/javase/tutorial/java/javaOO/enum.html[enum] within +org.elasticsearch.spark.rdd+ package which identifies its type - +id+, +ttl+, +version+, etc...
Thus an +RDD+ keys can be a +Map+ containing the +Metadata+ for each document and its associated values. If +RDD+ key is not of type +Map+, {esh} will consider the object as representing the document id and use it accordingly.
This sounds more complicated than it is, so let us see some examples.

.Scala

Pair ++RDD++s, or simply put ++RDD++s with the signature +RDD[(K,V)]+ can take advantage of the +saveToEsWithMeta+ methods that are available either through the _implicit_ import of +org.elasticsearch.spark+ package or +EsSpark+ object.
To manually specify the id for each document, simply pass in the +Object+ (not of type +Map+) in your +RDD+:

[source,scala]
----
val otp = Map("iata" -> "OTP", "name" -> "Otopeni")
val muc = Map("iata" -> "MUC", "name" -> "Munich")
val sfo = Map("iata" -> "SFO", "name" -> "San Fran")

// instance of SparkContext
val sc = ...

val airportsRDD<1> = sc.makeRDD(Seq((1, otp), (2, muc), (3, sfo)))  <2>
pairRDD.saveToEsWithMeta<3>(airportsRDD, "airports/2015")
----

<1> +airportsRDD+ is a __key-value__ pair +RDD+; it is created from a +Seq+ of ++tuple++s
<2> The key of each tuple within the +Seq+ represents the _id_ of its associated value/document; in other words, document +otp+ has id +1+, +muc+ +2+ and +sfo+ +3+
<3> Since +airportsRDD+ is a pair +RDD+, it has the +saveToEsWithMeta+ method available. This tells {esh} to pay special attention to the +RDD+ keys and use them as metadata, in this case as document ids. If +saveToEs+ would have been used instead, then {esh} would consider the +RDD+ tuple, that is both the key and the value as part of the document.

When more than just the id needs to be specified, one should use a +scala.collection.Map+ with keys of type +org.elasticsearch.spark.rdd.Metadata+:

[source,scala]
----
import org.elasticsearch.spark.rdd.Metadata._          <1>

val otp = Map("iata" -> "OTP", "name" -> "Otopeni")
val muc = Map("iata" -> "MUC", "name" -> "Munich")
val sfo = Map("iata" -> "SFO", "name" -> "San Fran")

// metadata for each document
// note it's not required for them to have the same structure
val otpMeta = Map(ID -> 1, TTL -> "3h")                <2>
val mucMeta = Map(ID -> 2, VERSION -> "23")            <3>
val sfoMeta = Map(ID -> 3)                             <4>

// instance of SparkContext
val sc = ...

val airportsRDD = sc.makeRDD<5>(Seq((otpMeta, otp), (mucMeta, muc), (sfoMeta, sfo)))
pairRDD.saveToEsWithMeta(airportsRDD, "airports/2015") <6>
----

<1> Import the +Metadata+ enum
<2> The metadata used for +otp+ document. In this case, +ID+ with a value of 1 and +TTL+ with a value of +3h+
<3> The metadata used for +muc+ document. In this case, +ID+ with a value of 2 and +VERSION+ with a value of +23+
<4> The metadata used for +sfo+ document. In this case, +ID+ with a value of 3
<5> The metadata and the documents are assembled into a _pair_ +RDD+
<6> The +RDD+ is saved accordingly using the +saveToEsWithMeta+ method

.Java

In a similar fashion, on the Java side, +JavaEsSpark+ provides +saveToEsWithMeta+ methods that are applied to +JavaPairRDD+ (the equivalent in Java of +RDD[(K,V)]+). Thus to save documents based on their ids one can use:

[source,java]
----
import org.elasticsearch.spark.java.api.JavaEsSpark;

// data to be saved
Map<String, ?> otp = ImmutableMap.of("iata", "OTP", "name", "Otopeni");
Map<String, ?> jfk = ImmutableMap.of("iata", "JFK", "name", "JFK NYC");

JavaSparkContext jsc = ...

// create a pair +RDD+ between the id and the docs
JavaPairRDD<?, ?> pairRdd = jsc.parallelizePairs<1>(ImmutableList.of(
        new Tuple2<Object, Object>(1, otp),          <2>
        new Tuple2<Object, Object>(2, jfk)));        <3>
JavaEsSpark.saveToEsWithMeta(pairRDD, target);       <4>
----

<1> Create a +JavaPairRDD+ by using Scala +Tuple2+ class wrapped around the document id and the document itself
<2> Tuple for the first document wrapped around the id (+1+) and the doc (+otp+) itself
<3> Tuple for the second document wrapped around the id (+2+) and +jfk+
<4> The +JavaPairRDD+ is saved accordingly using the keys as a id and the values as documents

When more than just the id needs to be specified, one can chose to use a +java.util.Map+ populated with keys of type +org.elasticsearch.spark.rdd.Metadata+:

[source,java]
----
import org.elasticsearch.spark.java.api.JavaEsSpark;
import org.elasticsearch.spark.rdd.Metadata;          <1>

import static org.elasticsearch.spark.rdd.Metadata.*; <2>

// data to be saved
Map<String, ?> otp = ImmutableMap.of("iata", "OTP", "name", "Otopeni");
Map<String, ?> sfo = ImmutableMap.of("iata", "SFO", "name", "San Fran");

// metadata for each document
// note it's not required for them to have the same structure
Map<Metadata, Object> otpMeta<3> = ImmutableMap.<Metadata, Object><4> of(ID, 1, TTL, "1d");
Map<Metadata, Object> sfoMeta<5> = ImmutableMap.<Metadata, Object> of(ID, "2", VERSION, "23");

JavaSparkContext jsc = ...

// create a pair +RDD+ between the id and the docs
JavaPairRDD<?, ?> pairRdd = jsc.parallelizePairs<(ImmutableList.of(
        new Tuple2<Object, Object>(otpMeta, otp),    <6>
        new Tuple2<Object, Object>(sfoMeta, sfo)));  <7>
JavaEsSpark.saveToEsWithMeta(pairRDD, target);       <8>
----

<1> +Metadata+ +enum+ describing the document metadata that can be declared
<2> static import for the +enum+ to refer to its values in short format (+ID+, +TTL+, etc...)
<3> Metadata for +otp+ document
<4> Boiler-plate construct for forcing the +of+ method generic signature
<5> Metadata for +sfo+ document
<6> Tuple between +otp+ (as the value) and its metadata (as the key)
<7> Tuple associating +sfo+ and its metadata
<8> +saveToEsWithMeta+ invoked over the +JavaPairRDD+ containing documents and their respective metadata

[[spark-read]]
[float]
==== Reading data from {es}

For reading, one should define the {es} +RDD+ that _streams_ data from {es} to Spark.

.Scala

Similar to writing, the +org.elasticsearch.spark+ package, enriches the +SparkContext+ API with +esRDD+ methods:

[source,scala]
----
import org.apache.spark.SparkContext    <1>
import org.apache.spark.SparkContext._

import org.elasticsearch.spark._        <2>

...

val conf = ...
val sc = new SparkContext(conf)         <3>

val +RDD+ = sc.esRDD("radio/artists")     <4>
----

<1> Spark Scala imports
<2> {eh} Scala imports
<3> start Spark through its Scala API
<4> a dedicated `RDD` for {es} is created for index `radio/artists`

The method can be overloaded to specify an additional query or even a configuration `Map` (overriding `SparkConf`):

[source,scala]
----
...
import org.elasticsearch.spark._

...
val conf = ...
val sc = new SparkContext(conf)

sc.esRDD("radio/artists", "?me*") <1>
----

<1> create an `RDD` streaming all the documents matching `me*` from index `radio/artists`

The documents from {es} are returned, by default, as a +Tuple2+ containing as the first element the document id and the second element the actual document represented through Scala http://docs.scala-lang.org/overviews/collections/overview.html[collections], namely one `Map[String, Any]`where the keys represent the field names and the value their respective values.

.Java

Java users have a dedicated `JavaPairRDD` that works the same as its Scala counterpart however the returned +Tuple2+ values (or second element) returns the documents as native, `java.util` collections.

[source,java]
----
import org.apache.spark.api.java.JavaSparkContext;               <1>
import org.elasticsearch.spark.java.api.JavaEsSpark;             <2>
...

SparkConf conf = ...
JavaSparkContext jsc = new JavaSparkContext(conf);               <3>

JavaPairRDD<String, Map<String, Object>> esRDD = 
                        JavaEsSpark.esRDD(jsc, "radio/artists"); <4>
----

<1> Spark Java imports
<2> {eh} Java imports
<3> start Spark through its Java API
<4> a dedicated `JavaPairRDD` for {es} is created for index `radio/artists`

In a similar fashion one can use the overloaded `esRDD` methods to specify a query or pass a `Map` object for advanced configuration.
Let us see how this looks like, but this time around using http://docs.oracle.com/javase/1.5.0/docs/guide/language/static-import.html[Java static imports] - further more, let us discard the documents ids and retrieve only the +RDD+ values:

[source,java]
----
import static org.elasticsearch.spark.java.api.JavaEsSpark.*;          <1>

...
JavaRDD<Map<String, Object>> esRDD = 
                        esRDD(jsc, "radio/artists", "?me*"<2>).values()<3>; 
----

<1> statically import `JavaEsSpark` class
<2> create an `RDD` streaming all the documents starting with `me` from index `radio/artists`. Note the method does not have to be fully qualified due to the static import
<3> return only _values_ of the +PairRDD+ - hence why the result is of type +JavaRDD+ and _not_ +JavaPairRDD+

By using the `JavaEsSpark` API, one gets a hold of Spark's dedicated `JavaPairRDD` which are better suited in Java environments than the base `RDD` (due to its Scala
signatures). Moreover, the dedicated +RDD+ returns {es} documents as proper Java collections so one does not have to deal with Scala collections (which
is typically the case with ++RDD++s). This is particularly powerful when using Java 8, which we strongly recommend as its 
http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html[lambda expressions] make collection processing _extremely_ concise.

To wit, let us assume one wants to filter the documents from the +RDD+ and return only those that contain a value that contain +mega+ (please ignore the fact one can and should do the filtering directly through {es}).

In versions prior to Java 8, the code would look something like this:
[source, java]
----
JavaRDD<Map<String, Object>> esRDD = 
                        esRDD(jsc, "radio/artists", "?me*").values();
JavaRDD<Map<String, Object>> filtered = esRDD.filter(
    new Function<Map<String, Object>, Boolean>() {
      @Override
      public Boolean call(Map<String, Object> map) throws Exception {
          returns map.contains("mega");
      }
    });
----

with Java 8, the filtering becomes a one liner:

[source,java]
----
JavaRDD<Map<String, Object>> esRDD = 
                        esRDD(jsc, "radio/artists", "?me*").values();
JavaRDD<Map<String, Object>> filtered = esRDD.filter(doc -> 
                                                doc.contains("mega"));
----

[[spark-sql]]
[float]
=== Spark SQL support

added[2.1]

[quote, Spark website]
____
http://spark.apache.org/sql/[Spark SQL] allows relational queries expressed in SQL, HiveQL, or Scala to be executed using Spark. At the core of this component is a new type of +RDD+, SchemaRDD. A SchemaRDD is similar to a table in a traditional relational database. [...] Spark SQL is currently an alpha component. 
____
On top of the core Spark support, {eh} also provides integration with Spark SQL. In other words, {es} becomes a _native_ source for Spark SQL so that data can be indexed and queried from Spark SQL _transparently_.

IMPORTANT: Spark SQL works with _structured_ data - in other words, all entries are expected to have the _same_ structure (same number of fields, of the same type and name). Using unstructured data (documents with different
structures) is _not_ supported and will cause problems. For such cases, use ++PairRDD++s.

IMPORTANT: Spark SQL is a young component, going through significant changes between releases. While we strive to maintain compatibility going forward, the functionality between different Spark releases may change.

IMPORTANT: Spark SQL is a _separate_ component from Spark _core_. To use it, make sure to properly add it to your classpath - please see the <<requirements>> page for more details.

Spark SQL support is available under +org.elasticsearch.spark.sql+ package.

[float]
[[spark-sql-write]]
==== Writing +SchemaRDD+ to {es}

With {eh}, ++SchemaRDD++s can be indexed to {es}.

.Scala

In Scala, simply import +org.elasticsearch.spark.sql+ package which enriches the given +SchemaRDD+ class with +saveToEs+ methods; while these have the same signature as the +org.elasticsearch.spark+ package, they are designed for +SchemaRDD+ implementations:

[source,scala]
----
// reusing the example from Spark SQL documentation

import org.apache.spark.sql.SQLContext    <1>
import org.apache.spark.sql.SQLContext._  

import org.elasticsearch.spark.sql._      <2>

...

// sc = existing SparkContext
val sqlContext = new SQLContext(sc)

// case class used to define the +RDD+ schema
case class Person(name: String, surname: String, age: Int)

//  create SchemaRDD
val people = sc.textFile("people.txt")    <3>
        .map(_.split(","))
        .map(p => Person(p(0), p(1), p(2).trim.toInt))


people.saveToEs("spark/people")           <4>
----

<1> Spark SQL package import
<2> {eh} Spark package import
<3> Read a text file as _normal_ +RDD+ and map it to a +SchemaRDD+ (using the +Person+ case class)
<4> Index the resulting +SchemaRDD+ to {es} through the +saveToEs+ method

.Java

In a similar fashion, for Java usage the dedicated package +org.elasticsearch.spark.sql.java.api+ provides similar functionality through the +JavaEsSparkSQL+ :

[source,java]
----
import org.apache.spark.sql.api.java.*;                      <1>
import org.elasticsearch.spark.sql.java.api.JavaEsSparkSQL;  <2>
...

JavaSchemaRDD people = ...
JavaEsSparkSQL.saveToEs("spark/people");                     <3>
----

<1> Spark SQL Java imports
<2> {eh} Spark SQL Java imports
<3> index the +JavaSchemaRDD+ in {es} under +spark/people+

Again, with Java 5 _static_ imports this can be further simplied to:

[source,java]
----
import static org.elasticsearch.spark.sql.java.api.JavaEsSparkSQL; <1>
...
saveToEs("spark/people");                                          <2>
----

<1> statically import +JavaEsSparkSQL+
<2> call +saveToEs+ method without having to type +JavaEsSpark+ again

IMPORTANT: For maximum control over the mapping of your +SchemaRDD+ in {es}, it is highly recommended to create the mapping before hand. See <<mapping, this>> chapter for more information.

[[spark-sql-json]]
[float]
==== Writing existing JSON to {es}

When using Spark SQL, if the input data is in JSON format, simply convert it to +SchemaRDD+ (as described in Spark https://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets[documentation]) through +SQLContext+/+JavaSQLContext+ +jsonFile+ methods.

[[spark-sql-read]]
[float]
==== Using pure SQL to read from {es}

IMPORTANT: Available in Apache SparkSQL 1.2 (or higher)
IMPORTANT: The index and its mapping, have to exist prior to creating the temporary table

SparkSQL 1.2 http://spark.apache.org/releases/spark-release-1-2-0.html[introduced] a new https://github.com/apache/spark/pull/2475[API] for reading from external data sources, which is supported by {eh} 
simplifying the SQL configured needed for interacting with Elasticsearch. Further more, behind the scenes it understands the operations executed by Spark and thus can optimize the data and queries made (such as filtering or pruning), 
improving performance.

To use it, declare a Spark temporary table backed by {eh}:

[source,scala]
----
sqlContext.sql(
   "CREATE TEMPORARY TABLE myIndex    " + <1>
   "USING org.elasticsearch.spark.sql " + <2>
   "OPTIONS (resource 'spark/index')  " ) <3>
----

<1> Spark's temporary table name 
<2> +USING+ clause identifying the data source provider, in this case +org.elasticsearch.spark.sql+
<3> {eh} <<configuration,configuration options>>, the mandatory one being +resource+. One can use the +es.+ prefix or skip it for convenient.

Once defined, the schema is picked up automatically. So one can issue queries, right away:

[source,sql]
----
val allRDD = sqlContext.sql("SELECT * FROM myIndex WHERE id <= 10")
----

As {eh} is aware of the queries being made, it can _optimize_ the requests done to {es}. For example, given the following query:

[source,sql]
----
val nameRDD = sqlContext.sql("SELECT name FROM myIndex WHERE id >=1 AND id <= 10")
----

it knows only the +name+ and +id+ fields are required (the first to be returned to the user, the second for Spark's internal filtering) and thus will ask _only_ for this data, making the queries quite efficient.

[float]
==== Reading ++SchemaRDD++s from {es}

As you might have guessed, one can define a +SchemaRDD+ backed by {es} documents. Or even better, have them backed by a query result, effectively creating dynamic, real-time _views_ over your data.

.Scala

Through the +org.elasticsearch.spark.sql+ package, +esRDD+ methods are available on the +SQLContext+ API:

[source,scala]
----
import org.apache.spark.sql.SQLContext        <1>

import org.elasticsearch.spark.sql._          <2>
...

val sql = new SQLContext(sc)

val people = sqlContext.esRDD("spark/people") <3>

// check the associated schema
println(people.schema)                        <4>
// root
//  |-- name: string (nullable = true)
//  |-- surname: string (nullable = true)
//  |-- age: long (nullable = true)           <5>
----

<1> Spark SQL Scala imports
<2> {eh} SQL Scala imports
<3> create a +SchemaRDD+ backed by the +spark/people+ index in {es}
<4> the +SchemaRDD+ associated schema discovered from {es}
<5> notice how the +age+ field was transformed into a +Long+ when using the default {es} mapping as discussed in the <<mapping>> chapter.

And just as with the Spark _core_ support, additional parameters can be specified such as a query. This is quite a _powerful_ concept as one can filter the data at the source ({es}) and use Spark only on the results:

[source,scala]
----
// get only the Smiths
val smiths = sqlContext.esRDD("spark/people","?q=Smith" <1>)
----

<1> {es} query whose results comprise the +RDD+

.Java

[source,java]
----
import org.apache.spark.sql.api.java.JavaSQLContext;          <1>
import org.elasticsearch.spark.sql.java.api.JavaEsSparkSQL;   <2>
...
JavaSQLContext sql = new JavaSQLContext(sc);

JavaSchemaRDD people = JavaSQLContext.esRDD("spark/people");  <3>
----

<1> Spark SQL import
<2> {eh} import
<3> create a Java +SchemaRDD+ backed by an {es} index

Better yet, the +JavaSchemaRDD+ can be backed by a query result:

[source,java]
----
JavaSchemaRDD people = JavaSQLContext.esRDD("spark/people", "?q=Smith"  <1>); 
----

<1> {es} query backing the {eh} +SchemaRDD+

[[spark-mr]]
[float]
=== Using the {mr} layer

Another way of using Spark with {es} is through the {mr} layer, that is by leveraging the dedicate +Input/OuputFormat+ in {eh}. However, unless one is stuck on 
{eh} 2.0, we _strongly_ recommend using the native integration as it offers significantly better performance and flexibility.

[float]
==== Configuration

Through {eh}, Spark can integrate with {es} through its dedicated `InputFormat`, and in case of writing, through `OutputFormat`. These are described at length in the <<mapreduce, {mr}>> chapter so please refer to that for an in-depth explanation.

In short, one needs to setup a basic Hadoop +Configuration+ object with the target {es} cluster and index, potentially a query, and she's good to go.

From Spark's perspective, the only thing required is setting up serialization - Spark relies by default on Java serialization which is convenient but fairly inefficient. This is the reason why Hadoop itself introduced its own serialization mechanism and its own types - namely ++Writable++s. As such, +InputFormat+ and ++OutputFormat++s are required to return +Writables+ which, out of the box, Spark does not understand.
The good news is, one can easily enable a different serialization (https://github.com/EsotericSoftware/kryo[Kryo]) which handles the conversion automatically and also does this quite efficiently.

[source,java]
----
SparkConf sc = new SparkConf(); //.setMaster("local");
sc.set("spark.serializer", KryoSerializer.class.getName()); <1>

// needed only when using the Java API
JavaSparkContext jsc = new JavaSparkContext(sc);
----

<1> Enable the Kryo serialization support with Spark

Or if you prefer Scala

[source,scala]
----
val sc = new SparkConf(...)
sc.set("spark.serializer", classOf[KryoSerializer].getName) <1>
----

<1> Enable the Kryo serialization support with Spark

Note that the Kryo serialization is used as a work-around for dealing with +Writable+ types; one can choose to convert the types directly (from +Writable+ to +Serializable+ types) - which is fine however for getting started, the one liner above seems to be the most effective.

[float]
==== Reading data from {es}

To read data, simply pass in the `org.elasticsearch.hadoop.mr.EsInputFormat` class - since it supports both the `old` and the `new` {mr} APIs, you are free to use either method on ++SparkContext++'s, +hadoopRDD+ (which we recommend for conciseness reasons) or +newAPIHadoopRDD+. Which ever you chose, stick with it to avoid confusion and problems down the road.

[float]
===== 'Old' (`org.apache.hadoop.mapred`) API

[source,java]
----
JobConf conf = new JobConf();                             <1>
conf.set("es.resource", "radio/artists");                 <2>
conf.set("es.query", "?q=me*");                           <3>

JavaPairRDD esRDD = jsc.hadoopRDD(conf, EsInputFormat.class, 
                          Text.class, MapWritable.class); <4>
long docCount = esRDD.count();
----

<1> Create the Hadoop object (use the old API)
<2> Configure the source (index)
<3> Setup the query (optional)
<4> Create a Spark +RDD+ on top of {es} through `EsInputFormat` - the key represent the doc id, the value the doc itself

The Scala version is below:

[source,scala]
----
val conf = new JobConf()                                   <1>
conf.set("es.resource", "radio/artists")                   <2>
conf.set("es.query", "?q=me*")                             <3>
val esRDD = sc.hadoopRDD(conf, 
                classOf[EsInputFormat[Text, MapWritable]], <4>
                classOf[Text], classOf[MapWritable]))
val docCount = esRDD.count();
----

<1> Create the Hadoop object (use the old API)
<2> Configure the source (index)
<3> Setup the query (optional)
<4> Create a Spark +RDD+ on top of {es} through `EsInputFormat`

[float]
===== 'New' (`org.apache.hadoop.mapreduce`) API

As expected, the `mapreduce` API version is strikingly similar - replace +hadoopRDD+ with +newAPIHadoopRDD+ and +JobConf+ with +Configuration+. That's about it.

[source,java]
----
Configuration conf = new Configuration();       <1>
conf.set("es.resource", "radio/artists");       <2>
conf.set("es.query", "?q=me*");                 <3>

JavaPairRDD esRDD = jsc.newAPIHadoopRDD(conf, EsInputFormat.class, 
                Text.class, MapWritable.class); <4>
long docCount = esRDD.count();
----

<1> Create the Hadoop object (use the new API)
<2> Configure the source (index)
<3> Setup the query (optional)
<4> Create a Spark +RDD+ on top of {es} through `EsInputFormat` - the key represent the doc id, the value the doc itself

The Scala version is below:

[source,scala]
----
val conf = new Configuration()                             <1>
conf.set("es.resource", "radio/artists")                   <2>
conf.set("es.query", "?q=me*")                             <3>
val esRDD = sc.newAPIHadoopRDD(conf, 
                classOf[EsInputFormat[Text, MapWritable]], <4>
                classOf[Text], classOf[MapWritable]))
val docCount = esRDD.count();
----

<1> Create the Hadoop object (use the new API)
<2> Configure the source (index)
<3> Setup the query (optional)
<4> Create a Spark +RDD+ on top of {es} through `EsInputFormat`

