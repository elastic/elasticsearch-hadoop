[[configuration]]
== Configuration

{eh} behavior can be customized through the properties below, typically by setting them on the target job Hadoop `Configuration`. However some of them can be specified through other means depending on the library used (see the relevant section).

****
{eh} uses the same conventions and reasonable defaults as {es} so you can give it a try out without bothering with the configuration. Most of the time, these defaults are just fine for running a production cluster; if you are fine-tunning your cluster or wondering about the effect of certain configuration option, please _do ask_ for more information.
****

NOTE: All configuration properties start with the `es` prefix. The namespace `es.internal` is reserved by the library for its internal use and should _not_ be used by the user at any point.

[float]
=== Required settings

`es.resource`::
{es} resource location, where data is read _and_ written to. Requires the format `<index>/<type>` (relative to the {es} host/port (see <<cfg-network,below>>))).

[source,ini]
----
es.resource = twitter/tweet   # index 'twitter', type 'tweet'
----

`es.resource.read` (defaults to `es.resource`)::
{es} resource used for reading (but not writing) data. Useful when reading and writing data to different {es} indices within the _same_ job. Typically set automatically (expect for the {mr} module which requires manual configuration).

`es.resource.write`(defaults to `es.resource`):: 
{es} resource used for writing (but not reading) data. Used typically for __dynamic resource__ writes or when writing and reading data to different {es} indices within the _same_ job. Typically set automatically (expect for the {mr} module which requires manual configuration).

[[cfg-multi-writes]]
[float]
===== Dynamic/multi resource writes

For writing, {eh} allows the target resource to be resolved at runtime by using patterns (by using the `{<field-name>}` format), resolved at runtime based on the data being streamed to {es}. That is, one can save documents to a certain `index` or `type` based on one or multiple fields resolved from the document about to be saved.

For example, assuming the following document set (described here in JSON for readability - feel free to translate this into the actual Java objects):

[source,json]
----
{
    "media_type":"game",
    "title":"Final Fantasy VI",
    "year":"1994"
},
{
    "media_type":"book",
    "title":"Harry Potter",
    "year":"2010"
},
{
    "media_type":"music",
    "title":"Surfing With The Alien",
    "year":"1987"
}
----

to index each of them based on their `media_type` one would use the following pattern:

[source,ini]
----
# index the documents based on their type
es.resource.write = my-collection/{media_type}
----

which would result in `Final Fantasy VI` indexed under `my-collection/game`, `Harry Potter` under `my-collection/book` and `Surfing With The Alien` under `my-collection/music`.
For more information, please refer to the dedicated integration section.

IMPORTANT: Dynamic resources are supported _only_ for writing, for doing multi-index/types reads, use an appropriate http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-search.html[search query].

[float]
=== Essential settings

[[cfg-network]]
[float]
==== Network
`es.nodes` (default localhost)::
List of {es} nodes to connect to. When using {es} remotely, _do_ set this option. Note that the list does _not_ have to contain _every_ node inside the {es} cluster; these are discovered automatically by {eh} by default (see below). Each node can have its HTTP/REST port specified manually (e.g. `mynode:9600`).

`es.port` (default 9200)::
Default HTTP/REST port used for connecting to {es} - this setting is applied to the nodes in `es.nodes` that do not have any port specified.

[float]
==== Querying
`es.query` (default none)::
Holds the query used for reading data from the specified `es.resource`. By default it is not set/empty, meaning the entire data under the specified index/type is returned.
`es.query` can have three forms:

uri query;;
using the form `?uri_query`, one can specify a {ref}/search-uri-request.html[query string]. Notice the leading `?`.

query dsl;;
using the form `query_dsl` - note the query dsl needs to start with `{` and end with `}` as mentioned {ref}/search-request-body.html[here]

external resource;;
if none of the the two above do match, {eh} will try to interpret the parameter as a path within the HDFS file-system. If that is not the case, it will try to load the resource from the classpath or, if that fails, from the Hadoop `DistributedCache`. The resource should contain either a `uri query` or a `query dsl`.

To wit, here is an example:
[source,ini]
----
# uri (or parameter) query
es.query = ?q=costinl	

# query dsl
es.query = { "query" : { "term" : { "user" : "costinl" } } }

# external resource
es.query = org/mypackage/myquery.json
----

In other words, `es.query` is flexible enough so that you can use whatever search api you prefer, either inline or by loading it from an external resource.

TIP: We recommend using query dsl externalized in a file, included within the job jar (and thus available on its classpath). This makes it easy 
to identify, debug and organize your queries.
Through-out the documentation we use the uri query to save text and increase readability - real-life queries quickly become unwielding when used as uris.

[float]
==== Operation

`es.input.json` (default false)::
Whether the input is already in JSON format or not (the default). Please see the appropriate section of each
integration for more details about using JSON directly.

`es.write.operation` (default index)::
The write operation {eh} should peform - can be any of:
`index` (default);; new data is added while existing data (based on its id) is updated.
`create`;; adds new data - if the data already exists (based on its id), an exception is thrown.
`update`;; updates existing data (based on its id). If no data is found, an exception is thrown.

[float]
==== Mapping

`es.mapping.id` (default none)::
The document field/property name containing the document id.

`es.mapping.parent` (default none)::
The document field/property name containing the document parent. To specify a constant, use the `<CONSTANT>` format.

`es.mapping.version` (default none)::
The document field/property name containing the document version. To specify a constant, use the `<CONSTANT>` format.

`es.mapping.routing` (default none)::
The document field/property name containing the document routing. To specify a constant, use the `<CONSTANT>` format.

`es.mapping.ttl` (default none)::
The document field/property name containing the document time-to-live. To specify a constant, use the `<CONSTANT>` format.

`es.mapping.timestamp` (default none)::
The document field/property name containing the document timestamp. To specify a constant, use the `<CONSTANT>` format.

For example:
[source,ini]
----
# extracting the id from the field called 'uuid'
es.mapping.id = uuid

# specifying a parent with id '123'
es.mapping.parent = \<123>
----

[float]
=== Advanced settings

[[configuration-options-index]]
[float]
==== Index

`es.index.auto.create` (default yes)::
Whether {eh} should create an index (if its missing) when writing data to {es} or fail.

`es.index.read.missing.as.empty` (default no)::
Whether {eh} will allow reading of non existing indices (and return an empty data set) or not (and throw an exception)

`es.field.read.empty.as.null` (default yes)::
Whether {eh} will treat empty fields as `null`. This settings is typically not needed (as {eh} already handles the
null case) but is enabled for making it easier to work with text fields that haven't been sanitized yet.

`es.field.read.validate.presence` (default warn)::
To help out spot possible mistakes when querying data from Hadoop (which results in incorrect data being returned), {eh} can 
perform validation spotting missing fields and potential typos. Possible values are :
`ignore`;; no validation is performed
`warn`;; a warning message is logged in case the validation fails
`strict`;; an exception is thrown, halting the job, if a field is missing

[float]
==== Network

`es.nodes.discovery` (default true)::
Whether to discovery the nodes within the {es} cluster or only to use the ones given in `es.nodes` for metadata queries. Note that when reading and writing, {eh} uses the target index shards (and their hosting nodes), regardless of this setting.

`es.http.timeout` (default 1m)::
Timeout for HTTP/REST connections to {es}.

`es.http.retries` (default 3)::
Number of retries for estabilishing a (broken) http connection. The retries are applied for each _conversation_ with an {es} node. Once the retries are depleted, the connection will automatically be re-reouted to the next
available {es} node (based on the declaration of `es.nodes`, followed by the discovered nodes - if enabled).

`es.scroll.keepalive` (default 10m)::
The maximum duration of result scrolls between query requests.

`es.scroll.size` (default 50)::
Number of results/items returned by each individual scroll.

`es.action.heart.beat.lead` (default 15s)::
The lead to task timeout before {eh} informs Hadoop the task is still running to prevent task restart.

[float]
==== Proxy

`es.net.proxy.http.host`:: Http proxy host name
`es.net.proxy.http.port`:: Http proxy port
`es.net.proxy.http.user`:: Http proxy user name
`es.net.proxy.http.pass`:: Http proxy password
`es.net.proxy.http.use.system.props`(default yes):: Whether the use the system Http proxy properties (namely `http.proxyHost` and `http.proxyPort`) or not

`es.net.proxy.socks.host`:: Http proxy host name
`es.net.proxy.socks.port`:: Http proxy port
`es.net.proxy.socks.user`:: Http proxy user name
`es.net.proxy.socks.pass`:: Http proxy password
`es.net.proxy.http.use.system.props`(default yes):: Whether the use the system Socks proxy properties (namely `socksProxyHost` and `socksProxyHost`) or not

NOTE: {eh} allows proxy settings to be applied only to its connection using the setting above. Take extra care when there is already a JVM-wide proxy setting (typically through system properties) to avoid unexpected behavior.

[float]
==== Serialization

`es.batch.size.bytes` (default 1mb)::
Size (in bytes) for batch writes using {es} {ref}/docs-bulk.html[bulk] API. Note the bulk size is allocated _per task_ instance. Always multiply by the number of tasks within a Hadoop job to get the total bulk size at runtime hitting {es}.

`es.batch.size.entries` (default 1000)::
Size (in entries) for batch writes using {es} {ref}/docs-bulk.html[bulk] API - (0 disables it). Companion to `es.batch.size.bytes`, once one matches, the batch update is executed. Similar to the size, this setting is _per task_ instance; it gets multiplied at runtime by the total number of Hadoop tasks running.

`es.batch.write.refresh` (default true)::
Whether to invoke an {ref}/indices-refresh.html[index refresh] or not after a bulk update has been completed. Note this is called only after the entire write (meaning multiple bulk updates) have been executed.

`es.batch.write.retry.count` (default 3)::
Number of retries for a given batch in case {es} is overloaded and data is rejected. Note that only the rejected data is retried. If there is still data rejected after the retries have been performad, the Hadoop job is cancelled (and fails).

`es.batch.write.retry.wait` (default 10s)::
Time to wait between batch write retries.

`es.ser.writer.value.class` (default _depends on the library used_)::
Name of the `ValueReader` implementation for converting JSON to objects. This is set by the framework depending on the library ({mr}, Cascading, Hive, Pig, etc...) used.

`es.ser.reader.value.class` (default _depends on the library used_)::
Name of the `ValueWriter` implementation for converting objects to JSON. This is set by the framework depending on the library ({mr}, Cascading, Hive, Pig, etc...) used.

[[configuration-runtime]]
== Hadoop runtime options

When using {eh}, it is important to be aware of the following Hadoop configurations that can influence the way Map/Reduce tasks are executed and in return {eh}.

IMPORTANT: Unfortunately, these settings need to be setup *manually* *before* the job / script configuration. Since {eh} is called too late in the life-cycle, after the task has been tasks have been already dispatched and as such, cannot influence the execution anymore.

[float]
=== Speculative execution

[quote, Yahoo! developer network]
____
As most of the tasks in a job are coming to a close, http://developer.yahoo.com/hadoop/tutorial/module4.html#tolerance[speculative execution] will schedule redundant copies of the remaining tasks across several nodes which do not have other work to perform. Therefore, the same input can be processed multiple times in parallel, to exploit differences in machine capabilities.
____

In other words, speculative execution is an *optimization*, enabled by default, that allows Hadoop to create duplicates tasks of those which it considers hanged or slowed down. When doing data crunching or reading resources, having duplicate tasks is harmless and means at most a waste of computation resources; however when writing data to an external store, this can cause data corruption through duplicates or unnecessary updates.
Since the 'speculative execution' behavior can be triggered by external factors (such as network or CPU load which in turn cause false positive) even in stable environments (virtualized clusters are particularly prone to this) and has a direct impact on data, {eh} disables this optimization for data safety.

Speculative execution can be disabled for the map and reduce phase - we recommend disabling in both cases - by setting to `false` the following two properties:

`mapred.map.tasks.speculative.execution`
`mapred.reduce.tasks.speculative.execution`

One can either set the properties by name manually on the `Configuration`/`JobConf` client:

[source,java]
----
jobConf.setSpeculativeExecution(false);
// or
configuration.setBoolean("mapred.map.tasks.speculative.execution", false);
configuration.setBoolean("mapred.reduce.tasks.speculative.execution", false);
----

or by passing them as arguments to the command line:

[source,bash]
----
$ bin/hadoop jar -Dmapred.map.tasks.speculative.execution=false \
                 -Dmapred.reduce.tasks.speculative.execution=false <jar>
----

[[logging]]
== Logging

{eh} uses http://commons.apache.org/proper/commons-logging/[commons-logging] library, same as Hadoop, for its logging infrastructure and thus it shares the same configuration means. Out of the box, no configuration is required - by default, {eh} logs relevant information about the job progress at `INFO` level.

Configuring logging for Hadoop (or Cascading, Hive and Pig) is outside the scope of this documentation, however in short, at runtime, Hadoop relies on http://logging.apache.org/log4j/1.2/[log4j 1.2] as an actual logging implementation. In practice, this means adding the package name of interest and its level logging the `log4j.properties` file in the job classpath.
{eh} provides the following important packages:
[cols="^,^",options="header"]

|===
| Package | Purpose

|`org.elasticsearch.hadoop.cascading` 		| Cascading integration
|`org.elasticsearch.hadoop.hive`	  		| Apache Hive integration
|`org.elasticsearch.hadoop.mr`		  		| {mr} functionality
|`org.elasticsearch.hadoop.pig`		  		| Apache Pig integration
|`org.elasticsearch.hadoop.rest`	  		| REST/transport infrastructure
|`org.elasticsearch.hadoop.serialization`   | Serialization package

|===

The default logging level (`INFO`) is suitable for day-to-day use; if troubleshooting is needed, consider switching to `DEBUG` but be selective of the packages included. For low-level details, enable level `TRACE` however do remember that it will result in a significant amount of logging data which _will_ impact your job performance and environment.

To put everything together, if you want to enable `DEBUG` logging on the {mr} package make sure the `log4j.properties` (used by your environment):

[source,bash]
----
log4j.category.org.elasticsearch.hadoop.mr=DEBUG
----

TIP: See the log4j https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PropertyConfigurator.html#doConfigure%28java.lang.String,%20org.apache.log4j.spi.LoggerRepository%29[javadoc] for more information.