[[storm]]
== Apache Storm support

added[2.1]

[quote, Storm website]
____
http://storm.apache.org[Apache Storm] is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. 
____
With Storm, one can compute, transform and filter data typically in a streaming scenario. As opposed to the rest of the libraries mentioned in this documentation, Apache Storm is a computational framework that is not tied to {mr} itself however it does integrate with Hadoop, mainly through HDFS.

IMPORTANT: Since {eh} 5.0, Storm 1.0 or higher (older versions cannot be used unfortunately as Storm 1.x broke backwards compatibility though package renaming).

[[storm-installation]]
[float]
=== Installation

In order to use {eh}, its jar needs to be available in Storm's classpath. The Storm https://storm.apache.org/documentation/Documentation.html[documentation] covers this in detail but in short, one can either have the jar available on all Storm nodes or have {eh} part of the jar being deployed (which we recommend). The latter approach allows isolation between the jobs and since the jar is self-contained, can be easily be moved across environments without additional setup making it much more robust.

[[storm-configuration]]
[float]
=== Configuration

The Storm integration supports the configuration options described in the <<configuration>> chapter plus a few more that are specific to Storm, namely:

[[storm-cfg-spout]]
[float]
==== Spout specific
`es.storm.spout.reliable` (default false)::
Indicates whether the dedicated +EsSpout+ is _reliable_, that is replays the documents in case of failure or not. By default it is set to +false+ since replaying requires the documents to be kept in memory until are being acknowledged.

`es.storm.spout.fields` (default "")::
Specify what fields +EsSpout+ will declare in its topology and extract from the returned documents. By default is unset meaning the documents are returned as ++Map++s under the default field +doc+.

`es.storm.spout.reliable.queue.size` (default 0)::
Applicable only if +es.storm.spout.reliable+ is +true+. Sets the size of the queue which holds documents in memory to be replayed until they are acknowledged. By default, the queue is _unbounded_ (+0+) however in a production environment
it is indicated to limit the queue to limit the consumption of memory. If the queue is full, the +Bolt+ drops any incoming ++Tuple++s and throws an exception.

`es.storm.spout.reliable.retries.per.tuple` (default 5)::
Applicable only if +es.storm.spout.reliable+ is +true+. Set the number of retries (replays) of a failed tuple before giving up. Setting it to a negative value will cause the tuple to be replayed until acknowledged.

`es.storm.spout.reliable.handle.tuple.failure` (default abort)::
Applicable only if +es.storm.spout.reliable+ is +true+. Indicates how to handle failing tuples after the number of retries is depleted. Possible values are :
`ignore`;; the tuple is discarded
`warn`;; a warning message is logged and the tuple is discarded
`strict`;; an exception is thrown, aborting the current job



[[storm-cfg-bolt]]
[float]
==== Bolt specific
`es.storm.bolt.write.ack` (default false)::
Indicates whether the dedicated +EsBolt+ is _reliable_, that is acknowledges the +Tuple+ _after_ it is written to {es} instead of when it receives it. By default it is +false+. Note that turning this on increases the memory requirements of the +Bolt+ since it has to keep the data in memory until it is fully written.

`es.storm.bolt.flush.entries.size` (default 1000)::
The number of entries that trigger a _micro-batch_ write to {es}. By default, it uses the same value as `es.batch.size.entries` which, by default is `1000`.

`es.storm.bolt.tick.tuple.flush` (default true)::
Whether or not to flush the existing data if the +Bolt+ receives a https://storm.incubator.apache.org/apidocs/[Tick] tuple. This _heart-beat_-like mechanism goes hand in hand with the flush limit above to create both a time and size trigger.
When using Storm's internal ticks, remember to set the tick interval:
[source,java]
----
// tick every 2 seconds
builder.setBolt(...).addConfiguration(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, 2) 
----

[[storm-cfg-set]]
[float]
==== Setting the configuration
The configuration can be set through Storm's https://storm.incubator.apache.org/apidocs/index.html?backtype/storm/Config.html[Config] class, in which case they are available _globally_ or, individually for each +EsSpout+ and +EsBolt+ instance. In general, it is recommended to set globally _only_ the properties that apply to all the components and are unlikely to change:

[source,java]
----
Config conf = new Config();
conf.put("es.index.auto.create", "true");
StormSubmitter.submitTopology("myTopology", conf, topology);
----

For this reason, typically, one should use the _per-component_ configuration model since it allows different configurations to be used within the same Storm topology:

[source,java]
----
Map conf = new HashMap();
conf.put("es.index.auto.create", "true");
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("esSpout", new EsSpout("index/type", conf));
----

[float]
[[storm-write]]
=== Writing data to {es}

Through {eh}, {es} is exposed to Storm through a native +Bolt+, namely +org.elasticsearch.storm.EsBolt+ that writes the Storm ++Tuple++s to {es}:

[source,java]
----
import org.elasticsearch.storm.EsBolt; <1>

TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("spout", new RandomSentenceSpout(), 10);
builder.setBolt(
    "es-bolt",
    new EsBolt(<2>
        "storm/docs"<3>
    ),
    5)<4>
    .shuffleGrouping("spout");
----

<1> {eh} +EsBolt+ package import
<2> +EsBolt+ declaration
<3> Various constructors are available for +EsBolt+ - at least the {es} resource under which the data is indexed, is required
<4> The number of +EsBolt+ instances for this topology

IMPORTANT: The number of bolt instances depends highly on your topology and environment. In general a good rule of thumb is to take into account the number of the target index shards as well as the number of spouts sending data to it - a good formula is to take the minimum between the source spouts and the index shards; in this example 5. A high number of ++Bolt++s does not translate to a bigger through-put - make sure the ++Bolt++s are the bottleneck since increasing the number simply translates otherwise to wasted cycles.

For cases where the id (or other metadata fields like +ttl+ or +timestamp+) of the document needs to be specified, one can do so by setting the appropriate <<cfg-mapping, mapping>>, namely +es.mapping.id+. Thus assuming the documents contain a field called +sentenceId+ which is unique and is suitable for an identifier, one can update the job configuration as follows:

[source,java]
----
Map conf = new HashMap();
conf.put("es.mapping.id", "sentenceId");
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("esSpout", new EsSpout("index/type", conf));
----

[float]
[[storm-write-json]]
==== Writing existing JSON to {es}

If the data passed to Storm is already in JSON format, +EsBolt+ can pass it directly to {es} _without_ any transformation; the data is taken as is and sent over the wire. In such cases, one needs to indicate the JSON input by setting the `es.input.json` parameter to `true`. Furthermore, the +Bolt+ expects the receiving +Tuple+ to contain only _one_ value/field representing the JSON document. By default, common _textual_ types are recognized, such as +chararray+ or +bytearray+; otherwise it falls back to calling +toString+ to get a hold of the JSON content.

[source,java]
----
String json1 = "{\"reason\" : \"business\",\"airport\" : \"SFO\"}";  <1>
String json2 = "{\"participants\" : 5,\"airport\" : \"OTP\"}";

Map conf = new HashMap();
conf.put("es.input.json", "true"); <2>

TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("json-spout", new StringSpout(Arrays.asList(json1, json2)); <3>
builder.setBolt("es-bolt", new EsBolt("storm/json-trips", conf)) <4>
                                    .shuffleGrouping("json-spout");
----

<1> JSON document represented as a +String+
<2> Option indicating the input is in JSON format
<3> Basic +Spout+ which replays the given ++String++s as +Tuples+ with only one value
<4> Configure +EsBolt+ to process JSON - the same setting can be passed through the global +Conf+ object however it is typically convenient to define it _locally_

[float]
[[storm-write-dyn]]
==== Writing to dynamic/multi-resources

In cases where the data needs to be indexed based on its content, one can choose the target index based on a +Tuple+ field.  Reusing the aforementioned <<cfg-multi-writes,media example>>, one can _partition_ the documents based on their type. Assuming the document tuple contains fields +media_type+, +title+ and +year+ one can index them as follows:

[source, java]
----
builder.setBolt("es-bolt", 
    new EsBolt("my-collection-{media_type}/doc") <1>
).shuffleGrouping("spout");
----

<1> Resource pattern using field +type+

For each tuple about to be written, {eh} will extract the +type+ field and use its value to determine the target resource. The functionality is also available when dealing with raw JSON - in this case, the value will be extracted from the JSON document itself.

The functionality is also available when dealing with raw JSON - in this case, the value will be extracted from the JSON document itself. Assuming the JSON source contains documents with the following structure:

[source,js]
----
{
    "media_type":"game",<1>
    "title":"Final Fantasy VI",
    "year":"1994"
}
----

<1> field within the JSON document that will be used by the pattern

the +EsBolt+ with the configuration:

[source, java]
----
Map conf = new HashMap();
conf.put("es.input.json", "true"); <1>

builder.setBolt("es-bolt", 
    new EsBolt(
        "my-collection-{media_type}-{year}/doc",<2>
        conf) <3>
    ).shuffleGrouping("spout");
----

<1> Option indicating the input is in JSON format
<2> Resource pattern
<3> Pass configuration to +EsBolt+ to indicate the JSON input


[float]
[[storm-read]]
==== Reading data from {es}

As you can expect, for reading data (typically executing queries) {eh} offers a dedicated +Spout+ through +org.elasticsearch.storm.EsSpout+ which executes the query in {es} and _streams_ the results back to {st}:

[source,java]
----
import org.elasticsearch.storm.EsSpout; <1>

TopologyBuilder builder = new TopologyBuilder();
builder.setSpout(
    "es-spout",
    new EsSpout(      <2>
        "storm/docs", <3>
        "?q=me*),     <4>
    5);               <5>
builder.setBolt("bolt", new PrinterBolt()).shuffleGrouping("es-spout");
----

<1> {eh} +EsSpout+ package import
<2> +EsSpout+ declaration
<3> The source {es} resource (index and type) for the data
<4> The query to execute (optional) - if no query is specified, the entire indexed data is streamed
<5> The number of +EsSpout+ instances for this topology. The number should *not* be greater than the number of shards available for an index; if it does, it just wastes CPU cycles without improving performance.

IMPORTANT: The number of +Spout+ instances depends highly on your topology and environment. Typically you should use the number of shards of your target data as an indicator - if you index has 5 shards, create 5 ++EsSpout++s; however sometimes the shards number might be considerably bigger than the number of ++Spout++s you can add to your {st} cluster; in that case, it is better to limit the number of ++EsSpout++ instances. Last but not least, adding more ++EsSpout++ instances than the number of shards of the source index does *not* improve performance; in fact the extra instances will just waste resources without processing anything.

[float]
===== Customizing +EsSpout+ fields

Since Storm requires each +Spout+ to declare its fields when creating a topology, by default +EsSpout+ declares for its tuples a generic +doc+ field containing the documents returned (one per tuple) from {es}. When dealing with structured data (documents sharing the same fields), one can configure the +EsSpout+ to _declare_ as fields the document properties effectively _unwrapping_ the document as a +Tuple+. By setting up +es.storm.spout.fields+, +EsSpout+ will use them indicate to the Storm topology the tuple content and extract them from the returned document.

For example if the {es} documents contain 3 fields: +name+, +age+ and +gender+ by setting +es.storm.spout.fields+ to ++name, age, gender++, instead of returning a tuple with one field (+doc+, containing the document), a tuple containing
the three named fields (+name+, +age+ and +gender+) will be returned instead.
