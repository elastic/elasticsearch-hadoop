[[pig]]
== Apache Pig support

[quote, Pig website]
____
http://pig.apache.org/[Apache Pig] is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs.
____
It provides a high-level, powerful, _scripting_-like transformation language which gets compiled into {mr} jobs at runtime by the Pig 'compiler'. To simplify working with arbitrary data, Pig associates a 'schema' (or type information) with each data set for validation and performance. This in turn, breaks it down into 'discrete' data types that can be transformed through various operators or custom functions (or http://pig.apache.org/docs/r0.11.1/udf.html[UDF]s). Data can be loaded from and stored to various storages such as the local file-system or HDFS, and with {eh} into {es} as well.

[[pig-installation]]
[float]
=== Installation

In order to use {eh}, its jar needs to be in Pig's classpath. There are various ways of making that happen though typically the http://pig.apache.org/docs/r0.11.1/basic.html#register[REGISTER] command is used:

[source,sql]
----
REGISTER /path/elasticsearch-hadoop.jar;
----

NOTE: the command expects a proper URI that can be found either on the local file-system or remotely. Typically it's best to use a distributed file-system (like HDFS or Amazon S3) and use that since the script might be executed
on various machines.

As an alternative, when using the command-line, one can register additional jars through the `-Dpig.additional.jars` option (that accepts an URI as well):

[source,bash]
----
$ pig -Dpig.additional.jars=/path/elasticsearch-hadoop.jar:<other.jars> script.pig
----

or if the jars are on HDFS

[source,bash]
----
$ pig \
-Dpig.additional.jars=hdfs://<cluster-name>:<cluster-port>/<path>/elasticsearch-hadoop.jar:<other.jars> \
script.pig
----

[[pig-configuration]]
[float]
=== Configuration

With Pig, one can specify the <<configuration,configuration>> properties (as an alternative to Hadoop `Configuration` object) as a constructor parameter when declaring `EsStorage`:

[source,sql]
----
STORE B INTO 'radio/artists'<1> USING org.elasticsearch.hadoop.pig.EsStorage
	         ('es.http.timeout = 5m<2>',
              'es.index.auto.create = false' <3>);
----

<1> {eh} configuration (target resource)
<2> {eh} option (http timeout)
<3> another {eh} configuration (disable automatic index creation)

TIP: To avoid having to specify the fully qualified class name (`org.elasticsearch.hadoop.pig.EsStorage`), consider using a shortcut through http://pig.apache.org/docs/r0.11.1/basic.html#define[`DEFINE`] command:

[source,sql]
----
DEFINE EsStorage org.elasticsearch.hadoop.pig.EsStorage();
----

Do note that it is possible (and recommended) to specify the configuration parameters to reduce script duplication, such as `es.query` or `es.mapping.names`:
[source,sql]
----
DEFINE EsStorage org.elasticsearch.hadoop.pig.EsStorage('my.cfg.param=value');
----
In fact, for the reminder of this section, the Pig alias will be implied and `EsStorage` will be used (instead of the fully qualified class name) for abbreviation.

[[pig-alias]]
[float]
=== Mapping

Out of the box, {eh} uses the Pig schema to map the data in {es}, using both the field names and types in the process. There are cases however when the names in Pig cannot
be used with {es} (invalid characters, existing names with different layout, etc...). For such cases, one can use the `es.mapping.names` setting which accepts a comma-separated list of names mapping in the following format: `Pig field name` : `{es} field name`

For example:

[source,sql]
----
STORE B INTO  '...' USING EsStorage('<1>es.mapping.names=date:@timestamp<2>, uRL:url<3>')
----

<1> name mapping for two fields
<2> Pig column `date` mapped in {es} to `@timestamp`
<3> Pig column `url` mapped in {es} to `url_123`

TIP: {es} accepts only lower-case field name and, as such, {eh} will always convert Pig column names to lower-case. Because Pig is **case sensitive**, {eh} handles the reverse
field mapping as well. It is recommended to use the default Pig style and use upper-case names only for commands and avoid mixed-case names.

[[pig-type-conversion]]
[float]
=== Type conversion

IMPORTANT: If automatic index creation is used, please review <<auto-mapping-type-loss,this>> section for more information.

Pig internally uses native java types for most of its types and {eh} abides to that convention.
[cols="^,^",options="header"]

|===
| Pig type | {es} type

| `null`            | `null`
| `chararray`       | `string`
| `int`             | `int`
| `long`            | `long`
| `double`          | `double`
| `float`           | `float`
| `bytearray`       | `binary`
| `tuple`           | `map`
| `bag`             | `array`
| `map`             | `map`

2+h| Available in Pig 0.10 or higher

| `boolean` 	    | `boolean`

2+h| Available in Pig 0.11 or higher

| `datetime` 	    | `date`

2+h| Available in Pig 0.12 or higher

| `biginteger` 	    | `not supported`
| `bigdecimal` 	    | `not supported`

|===

NOTE: While {es} understands the Pig types up to version 0.12, it is backwards compatible with Pig 0.9

[float]
=== Writing data to {es}

{es} is exposed as a native `Storage` to Pig so it can be used to store data into it:

[source,sql]
----
-- load data from HDFS into Pig using a schema
A = LOAD 'src/test/resources/artists.dat' USING PigStorage()
                    AS (id:long, name, url:chararray, picture: chararray);
-- transform data
B = FOREACH A GENERATE name, TOTUPLE(url, picture) AS links;
-- save the result to Elasticsearch
STORE B INTO 'radio/artists'<1> USING EsStorage(<2>);
----

<1> {es} resource (index and type) associated with the given storage
<2> additional configuration parameters can be passed here - in this case the defaults are used

[float]
=== Reading data from {es}

As you would expect, loading the data is straight forward:

[source,sql]
----
-- execute Elasticsearch query and load data into Pig
A = LOAD 'radio/artists'<1> USING EsStorage('es.query=?me*'<2>);
DUMP A;
----

<1> {es} resource
<2> search query to execute
