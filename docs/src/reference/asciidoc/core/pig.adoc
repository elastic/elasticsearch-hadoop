[[pig]]
== Apache Pig support

[quote, Pig website]
____
http://pig.apache.org/[Apache Pig] is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs.
____
It provides a high-level, powerful, _scripting_-like transformation language which gets compiled into {mr} jobs at runtime by the Pig 'compiler'. To simplify working with arbitrary data, Pig associates a 'schema' (or type information) with each data set for validation and performance. This in turn, breaks it down into 'discrete' data types that can be transformed through various operators or custom functions (or http://pig.apache.org/docs/r0.12.1/udf.html[UDF]s). Data can be loaded from and stored to various storages such as the local file-system or HDFS, and with {eh} into {es} as well.

[[pig-installation]]
[float]
=== Installation

In order to use {eh}, its jar needs to be in Pig's classpath. There are various ways of making that happen though typically the http://pig.apache.org/docs/r0.12.1/basic.html#register[REGISTER] command is used:

[source,sql]
----
REGISTER /path/elasticsearch-hadoop.jar;
----

NOTE: The command expects a proper URI that can be found either on the local file-system or remotely. Typically it's best to use a distributed file-system (like HDFS or Amazon S3) and use that since the script might be executed
on various machines.

As an alternative, when using the command-line, one can register additional jars through the `-Dpig.additional.jars` option (that accepts an URI as well):

[source,bash]
----
$ pig -Dpig.additional.jars=/path/elasticsearch-hadoop.jar:<other.jars> script.pig
----

or if the jars are on HDFS

[source,bash]
----
$ pig \
-Dpig.additional.jars=hdfs://<cluster-name>:<cluster-port>/<path>/elasticsearch-hadoop.jar:<other.jars> script.pig
----

[[pig-configuration]]
[float]
=== Configuration

With Pig, one can specify the <<configuration,configuration>> properties (as an alternative to Hadoop `Configuration` object) as a constructor parameter when declaring `EsStorage`:

[source,sql]
----
STORE B INTO 'radio/artists' <1>
       USING org.elasticsearch.hadoop.pig.EsStorage
             ('es.http.timeout = 5m', <2>
              'es.index.auto.create = false'); <3>
----

<1> {eh} configuration (target resource)
<2> {eh} option (http timeout)
<3> another {eh} configuration (disable automatic index creation)

TIP: To avoid having to specify the fully qualified class name (`org.elasticsearch.hadoop.pig.EsStorage`), consider using a shortcut through http://pig.apache.org/docs/r0.11.1/basic.html#define[`DEFINE`] command:

[source,sql]
----
DEFINE EsStorage org.elasticsearch.hadoop.pig.EsStorage();
----

Do note that it is possible (and recommended) to specify the configuration parameters to reduce script duplication, such as `es.query` or `es.mapping.names`:
[source,sql]
----
DEFINE EsStorage org.elasticsearch.hadoop.pig.EsStorage('my.cfg.param=value');
----

IMPORTANT: Pig definitions are replaced as are; even though the syntax allows _parametrization_, Pig will silently ignore any parameters outside the `DEFINE` declaration.

[[tuple-names]]
[float]
==== Tuple field names

Among the http://pig.apache.org/docs/r0.12.1/basic.html#data-types[various types] available in Pig, ++tuple++s are used the most. Tuples are defined as ``ordered sets of fields'' (e.g. `(19,2)`) however structurally they are shaped
as ordered _maps_ since each field has a name, which may be defined or not (e.g. `(field:19, another:2)`). The _ordered_ aspect is important and forces {eh} to use JSON arrays for tuples  (using JSON objects is not an option as it does not preserve ordering besides the fact that it requires keys/names which might be or not available in a tuple).
Obeying the rule of http://en.wikipedia.org/wiki/Principle_of_least_astonishment[least surprise], {eh} by default will _disregard_ a tuple's field names, both when writing and reading.

To change this behavior (which in effect means treating tuples as arrays of maps instead of arrays), use the boolean property `es.mapping.pig.tuple.use.field.names` (by default `false`) and set it to `true`.

The table below illustrates the difference between the two settings:

[cols="^,^,^",options="header"]
|===
| Tuple schema | Tuple value | Resulting JSON representation

3+h| `es.mapping.pig.tuple.use.field.names` *`false`* (default)

| `(foo: (nr:int, name:chararray))` | `(1,"kimchy")` | `{"foo":[1, "kimchy"]}`
| `(bar: (int, chararray))` | `(1,"kimchy")` | `{"bar":[1, "kimchy"]}`

3+h| `es.mapping.pig.tuple.use.field.names` *`true`*

| `(foo: (nr:int, name:chararray))` | `(1,"kimchy")` | `{"foo":[{"nr":1, "name":"kimchy"}]}`
| `(bar: (int, chararray))` | `(1,"kimchy")` | `{"bar":[{"val_0":1, "val_1":"name"}]}`

|===

IMPORTANT: When using tuples, it is _highly_ recommended to create the index mapping before-hand as it is quite common for tuples to contain mixed types (numbers, strings, other tuples, etc...) which, when mapped as an array (the default) can cause parsing errors (as the automatic mapping can infer the fields to be numbers instead of strings, etc...). In fact, the example above falls in this category since the tuple contains both a number (+1+) and a string (+"kimchy"+), which will the auto-detection to map both +foo+ and +bar+ as a number and thus causing an exception when encountering +"kimchy"+. Please refer to <<auto-mapping-type-loss,this>> for more information.
Additionally consider +breaking+/++flatten++ing the tuple into primitive/data atoms before sending the data off to Elasticsearch.

[[handling-splits]]
[float]
==== Reducers parallelism

By default, Pig will only use one reducer per job which in most cases is inefficient.  To address these issue:

Use the Parallel Features:: As explained in the http://pig.apache.org/docs/r0.13.0/perf.html#parallel[reference docs], out of the box Pig expects each reducer to process about 1 GB of data; unfortunately if the data is scattered 
around the network this becomes inefficient as the entire job is effectively serialized. Change this by increasing the number of reducers to map that of your shards through the +default_parallel+ property or +PARALLEL+ keyword:

[source,sql]
----
-- launch the Map/Reduce job with 5 reducers
SET default_parallel 5;
----
or by using the +PARALLEL+ keyword with +COGROUP+, +CROSS+, +DISTINCT+, +GROUP+, +JOIN+(inner), +JOIN+(outer) and ++ORDER BY++. 
[source,sql]
----
B = GROUP A BY t PARALLEL 18;
----

Disable split combination:: Out of the box Pig over-eagerly https://pig.apache.org/docs/r0.13.0/perf.html#combine-files[combines its input splits] even if it does not know how big they are. This again kills parallelism since it serializes the queries to {es} ; typically this looks as follows
in the logs:

[source,bash]
----
20yy-mm-dd hh:mm:ss,mss [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 25
20yy-mm-dd hh:mm:ss,mss [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1
----

Avoid this by setting +pig.noSplitCombination+ to +true+ (one can also use +pig.splitCombination+ to +false+ however we recommend the former) either by setting the property before invoking the script:

[source,bash]
----
pig -Dpig.noSplitCombination=true myScript.pig
----
in the Pig script itself:

[source,sql]
----
SET pig.noSplitCombination TRUE;
----
or through the global +pig.properties+ configuration in your Pig install:

[source,properties]
----
pig.noSplitCombination=true
----


Unfortunately {eh} cannot set these properties automatically so the user has to do that manually per script or making them global through the Pig configuration as described above.


[[pig-alias]]
[float]
=== Mapping

Out of the box, {eh} uses the Pig schema to map the data in {es}, using both the field names and types in the process. There are cases however when the names in Pig cannot
be used with {es} (invalid characters, existing names with different layout, etc...). For such cases, one can use the `es.mapping.names` setting which accepts a comma-separated list of mapped names in the following format: `Pig field name` : `Elasticsearch field name`

For example:

[source,sql]
----
STORE B INTO  '...' USING org.elasticsearch.hadoop.pig.EsStorage(
    'es.mapping.names=date:@timestamp, uRL:url')         <1>
----

<1> Pig column `date` mapped in {es} to `@timestamp`; Pig column `uRL` mapped in {es} to `url`

TIP: Since {eh} 2.1, the Pig schema case sensitivity is preserved to {es} and back. 

[float]
=== Writing data to {es}

{es} is exposed as a native `Storage` to Pig so it can be used to store data into it:

[source,sql]
----
-- load data from HDFS into Pig using a schema
A = LOAD 'src/test/resources/artists.dat' USING PigStorage()
                    AS (id:long, name, url:chararray, picture: chararray);
-- transform data
B = FOREACH A GENERATE name, TOTUPLE(url, picture) AS links;
-- save the result to Elasticsearch
STORE B INTO 'radio/artists'<1>
       USING org.elasticsearch.hadoop.pig.EsStorage(); <2>
----

<1> {es} resource (index and type) associated with the given storage
<2> additional configuration parameters can be passed inside the `()` - in this
case the defaults are used

For cases where the id (or other metadata fields like +ttl+ or +timestamp+) of the document needs to be specified, one can do so by setting the appropriate <<cfg-mapping, mapping>>, namely +es.mapping.id+. Following the previous example, to indicate to {es} to use the field +id+ as the document id, update the +Storage+ configuration:

[source,sql]
----
STORE B INTO 'radio/artists USING org.elasticsearch.hadoop.pig.EsStorage('es.mapping.id=id'...);
----

[float]
==== Writing existing JSON to {es}

When the job input data is already in JSON, {eh} allows direct indexing _without_ applying any transformation; the data is taken as is and sent directly to {es}. In such cases, one needs to indicate the json input by setting
the `es.input.json` parameter. As such, in this case {eh} expects to receive a tuple with a single field (representing the JSON document); the library will recognize common 'textual' types such as `chararray` or `bytearray` otherwise it just calls `toString` to get a hold of the JSON content.

.Pig types to use for JSON representation

[cols="^,^",options="header"]
|===
| `Pig type` | Comment 

| `bytearray`       | use this when the JSON data is represented as a `byte[]` or similar
| `chararray`       | use this if the JSON data is represented as a `String`
| _anything else_   | make sure the `toString()` returns the desired JSON document

|===
IMPORTANT: Make sure the data is properly encoded, in `UTF-8`. The field content is considered the final form of the document sent to {es}.

[source,sql]
----
A = LOAD '/resources/artists.json' USING PigStorage() AS (json:chararray);" <1>
STORE B INTO 'radio/artists' 
    USING org.elasticsearch.hadoop.pig.EsStorage('es.input.json=true'...); <2>
----

<1> Load the (JSON) data as a single field (`json`)
<2> Indicate the input is of type JSON.

[float]
==== Writing to dynamic/multi-resources

One can index the data to a different resource, depending on the 'row' being read, by using patterns. Reusing the aforementioned <<cfg-multi-writes,media example>>, one could configure it as follows:

[source,sql]
----
A = LOAD 'src/test/resources/media.dat' USING PigStorage()
            AS (name:chararray, type:chararray, year: chararray); <1>
STORE B INTO 'my-collection-{type}/doc' <2>
       USING org.elasticsearch.hadoop.pig.EsStorage();
----

<1> Tuple field used by the resource pattern. Any of the declared fields can be used.
<2> Resource pattern using field `type` - note that patterns should only be used in the index name


For each tuple about to be written, {eh} will extract the `type` field and use its value to determine the target resource.

The functionality is also available when dealing with raw JSON - in this case, the value will be extracted from the JSON document itself. Assuming the JSON source contains documents with the following structure:

[source,js]
----
{
    "media_type":"game",<1>
    "title":"Final Fantasy VI",
    "year":"1994"
}
----

<1> field within the JSON document that will be used by the pattern

the table declaration can be as follows:

[source,sql]
----
A = LOAD '/resources/media.json' USING PigStorage() AS (json:chararray);" <1>
STORE B INTO 'my-collection-{media_type}/doc' <2>
    USING org.elasticsearch.hadoop.pig.EsStorage('es.input.json=true');
----

<1> Schema declaration for the tuple. Since JSON input is used, the schema is simply a holder to the raw data
<2> Resource pattern relying on fields _within_ the JSON document and _not_ on the table schema

[float]
=== Reading data from {es}

As you would expect, loading the data is straight forward:

[source,sql]
----
-- execute Elasticsearch query and load data into Pig
A = LOAD 'radio/artists' <1>
    USING org.elasticsearch.hadoop.pig.EsStorage('es.query=?me*'); <2>
DUMP A;
----

<1> {es} resource
<2> search query to execute

IMPORTANT: Due to a https://issues.apache.org/jira/browse/PIG-3646[bug] in Pig, +LoadFunctions+ are not aware of any schema associated with them. This means +EsStorage+ is forced to fully parse the documents
from Elasticsearch before passing the data to Pig for projection. In practice, this has little impact as long as a document top-level fields are used; for nested fields consider extracting the values
yourself in Pig.


[float]
=== Reading data from {es} as JSON

In the case where the results from {es} need to be in JSON format (typically to be sent down the wire to some other system), one can instruct {eh} to return the data as is. By setting `es.output.json` to `true`, the connector will parse the response from {es}, identify the documents and, without converting them, return their content to the user as +String/chararray+ objects.


[[pig-type-conversion]]
[float]
=== Type conversion

IMPORTANT: If automatic index creation is used, please review <<auto-mapping-type-loss,this>> section for more information.

Pig internally uses native java types for most of its types and {eh} abides to that convention.
[cols="^,^",options="header"]

|===
| Pig type | {es} type

| `null`            | `null`
| `chararray`       | `string`
| `int`             | `int`
| `long`            | `long`
| `double`          | `double`
| `float`           | `float`
| `bytearray`       | `binary`
| `tuple`           | `array` or `map` (depending on <<tuple-names,this>> setting)
| `bag`             | `array`
| `map`             | `map`

2+h| Available in Pig 0.10 or higher

| `boolean`         | `boolean`

2+h| Available in Pig 0.11 or higher

| `datetime`        | `date`

2+h| Available in Pig 0.12 or higher

| `biginteger`      | `not supported`
| `bigdecimal`      | `not supported`

|===

NOTE: While {es} understands the Pig types up to version 0.12.1, it is backwards compatible with Pig 0.9

It is worth mentioning that rich data types available only in {es}, such as https://www.elastic.co/guide/en/elasticsearch/reference/2.1/geo-point.html[+GeoPoint+] or https://www.elastic.co/guide/en/elasticsearch/reference/2.1/geo-shape.html[+GeoShape+] are supported by converting their structure into the primitives available in the table above. For example, based on its storage a +geo_point+ might be
returned as a +chararray+ or a +tuple+.