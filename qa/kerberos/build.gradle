/*
 * Licensed to Elasticsearch under one or more contributor
 * license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright
 * ownership. Elasticsearch licenses this file to you under
 * the Apache License, Version 2.0 (the "License"); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */


import groovy.json.JsonSlurper
import org.apache.tools.ant.DefaultLogger
import org.elasticsearch.gradle.LoggedExec
import org.elasticsearch.gradle.Version
import org.elasticsearch.gradle.test.AntFixture
import org.elasticsearch.gradle.test.Fixture
import org.elasticsearch.gradle.test.NodeInfo
import org.elasticsearch.hadoop.gradle.fixture.hadoop.HadoopClusterConfiguration
import org.elasticsearch.hadoop.gradle.fixture.hadoop.HadoopServiceInfo
import org.elasticsearch.hadoop.gradle.fixture.hadoop.ServiceIdentifier
import org.elasticsearch.hadoop.gradle.tasks.ApacheMirrorDownload
import org.elasticsearch.hadoop.gradle.tasks.VerifyChecksums
import shadow.org.codehaus.plexus.util.Os

import java.nio.file.Paths
import java.security.MessageDigest

apply plugin: 'es.hadoop.build'

configurations {
    kdcFixture
}

dependencies {
    itestCompile project(":elasticsearch-hadoop-mr")
    itestCompile project(":elasticsearch-hadoop-mr").sourceSets.itest.runtimeClasspath

    kdcFixture project(':test:fixtures:minikdc')
}

String serverKeytabName = "http_es.build.elastic.co.keytab"

Map users = ["client":"password", "gmarx":"swordfish"]
Map keytabUsers = ["HTTP/es.build.elastic.co" : "es.keytab"]

// Configure MiniKDC
AntFixture kdcFixture = project.tasks.create('kdcFixture', AntFixture) {
    dependsOn project.configurations.kdcFixture
    executable = new File(project.runtimeJavaHome, 'bin/java')
    env 'CLASSPATH', "${ -> project.configurations.kdcFixture.asPath }"
    waitCondition = { fixture, ant ->
        // the kdc wrapper writes the ports file when
        // it's ready, so we can just wait for the file to exist
        return fixture.portsFile.exists()
    }

    final List<String> kdcFixtureArgs = []

    // Add provisioning lines first for sys properties
    users.forEach { k, v -> kdcFixtureArgs.add("-Des.fixture.kdc.user.${k}.pwd=${v}") }
    keytabUsers.forEach { k, v -> kdcFixtureArgs.add("-Des.fixture.kdc.user.${k}.keytab=${v}") }

    // Common options
    kdcFixtureArgs.add('org.elasticsearch.hadoop.test.fixture.minikdc.MiniKdcFixture')
    kdcFixtureArgs.add(baseDir.toString())

    args kdcFixtureArgs.toArray()
}

// KDC Fixture artifacts
java.nio.file.Path fixtureDir = project.buildDir.toPath().resolve("fixtures").resolve("kdcFixture")
java.nio.file.Path krb5Conf = fixtureDir.resolve("krb5.conf")
java.nio.file.Path esKeytab = fixtureDir.resolve("es.keytab")

// Configure ES with Kerberos Auth
esCluster {
    dependsOn(kdcFixture)

    // This may be needed if we ever run against java 9: --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED
    systemProperty("java.security.krb5.conf", krb5Conf.toString())
//    systemProperty("sun.security.krb5.debug", "true")

    // force localhost IPv4 otherwise it is a chicken and egg problem where we need the keytab for the hostname when starting the cluster
    // but do not know the exact address that is first in the http ports file
    setting 'http.host', '127.0.0.1'
    setting 'xpack.license.self_generated.type', 'trial'
    setting 'xpack.security.enabled', 'true'
    setting 'xpack.security.authc.realms.file.type', 'file'
    setting 'xpack.security.authc.realms.file.order', '0'
    setting 'xpack.ml.enabled', 'false'
    setting 'xpack.security.audit.enabled', 'true'
    // Kerberos realm
    setting 'xpack.security.authc.realms.kerberos.type', 'kerberos'
    setting 'xpack.security.authc.realms.kerberos.order', '1'
    setting 'xpack.security.authc.realms.kerberos.keytab.path', 'es.keytab'
    setting 'xpack.security.authc.realms.kerberos.krb.debug', 'true'
    setting 'xpack.security.authc.realms.kerberos.remove_realm_name', 'false'
    // Token realm
    setting 'xpack.security.authc.token.enabled', 'true'
    setting 'xpack.security.authc.token.timeout', '20m'

    setupCommand 'setupTestAdmin',
            'bin/elasticsearch-users', 'useradd', "test_admin", '-p', 'x-pack-test-password', '-r', 'superuser'

    extraConfigFile('es.keytab', esKeytab.toAbsolutePath())

    // Override the wait condition; Do the same wait logic, but pass in the test credentials for the API
    waitCondition = { node, ant ->
        File tmpFile = new File(node.cwd, 'wait.success')
        ant.get(src: "http://${node.httpUri()}/_cluster/health?wait_for_nodes=>=${numNodes}&wait_for_status=yellow",
                dest: tmpFile.toString(),
                username: 'test_admin',
                password: 'x-pack-test-password',
                ignoreerrors: true,
                retries: 10)
        return tmpFile.exists()
    }
}

// Configure Integration Test Task
Test integrationTest = project.tasks.findByName('integrationTest') as Test
integrationTest.dependsOn(kdcFixture)
integrationTest.finalizedBy(kdcFixture.getStopTask())
integrationTest.systemProperty("java.security.krb5.conf", krb5Conf.toString())
//integrationTest.systemProperty("sun.security.krb5.debug", "true")
integrationTest.systemProperty("es.net.http.auth.user", "test_admin")
integrationTest.systemProperty("es.net.http.auth.pass", "x-pack-test-password")


// =============================================================================
// Hadoop cluster testing
// =============================================================================

// Project instance available implicitly
String prefix = "hadoopFixture"
Task runner = integrationTest
HadoopClusterConfiguration config = new HadoopClusterConfiguration()

// FIXHERE: Inherent to Namenode only
config.addSetupCommand(new ServiceIdentifier(serviceName: "namenode"), "formatNamenode", ["bin/hdfs", "namenode", "-format"].toArray())

// Start cluster formation tasks code:

File sharedDir = new File(project.buildDir, "fixtures/shared")
Object startDependencies = config.dependencies

/*
 * First clean the environment
 */

// FIXHERE: Clean Shared

List<Task> startTasks = []
List<HadoopServiceInfo> nodes = []

// FIXHERE: Check node topologies
// - check node ranges, (num nodes >= bwc nodes)
// - require bwc version on non zero bwc node count

// Start Configure Distribution Dependency code section here:

// We need to configure the downloads here at this point for the versions of Hadoop artifacts that we're going to run against
// We don't need a configuration because we use the apache download mirrors
//Configuration currentDistro = project.configurations.create("${prefix}_hadoopDistro")
//configureDistributionDependency(project, 'tar', currentDistro, "Hadoop Version Here")
// FIXHERE: ClusterFormation Tasks gets its ES version to use from VersionProperties.elasticsearch, or from config.bwcVersion
Version hadoopFixtureVersion = new Version(2, 7, 7, null, false)

ApacheMirrorDownload downloadHadoop = project.tasks.create(name: "downloadHadoop#${hadoopFixtureVersion}", type: ApacheMirrorDownload) { ApacheMirrorDownload t ->
    t.dependsOn = startDependencies
    t.packagePath = 'hadoop/common'
    t.packageName = 'hadoop'
    t.version = "${hadoopFixtureVersion}"
    t.distribution = 'tar.gz'
    t.onlyIf { !t.outputFile().exists() }
}
VerifyChecksums newVerifySha1 = project.tasks.create(name: "verifyHadoop#${hadoopFixtureVersion}", type: VerifyChecksums) {
    VerifyChecksums v ->
        v.dependsOn downloadHadoop
        v.inputFile downloadHadoop.outputFile()
        v.checksum 'SHA-512', '17c8917211dd4c25f78bf60130a390f9e273b0149737094e45f4ae5c917b1174b97eb90818c5df068e607835120126281bcc07514f38bd7fd3cb8e9d3db1bdde'
}
startDependencies = newVerifySha1

// For each in topology
ServiceIdentifier serviceInstance = new ServiceIdentifier(serviceGroup: "hadoop", serviceName: "namenode")
HadoopServiceInfo node = new HadoopServiceInfo(config, serviceInstance, 0, project, prefix, hadoopFixtureVersion, sharedDir)
Object dependsOn = startTasks.empty ? startDependencies : startTasks.last() // We chain them in Hadoop because "topology"

// End Configure Distribution Dependency code section

// Start CONFIGURE NODE code:

// Configure Node
// Task: Clean home directory
// Task: Create CWD
// Task: Check Previous
// Task: Stop Previous
// Task: Extract Task
// Task: Write Config Task
// Don't need keystore stuff
// Don't need plugins or modules
// Task: Extra Config Files
// Task: Extra Start Up Commands
// Task: Start Task
// Task: Stop Task
// Return Start Task

static String taskName(String prefix, HadoopServiceInfo node, String action) {
    if (node.config.instances(node.serviceId) > 1) {
        return "${prefix}#${node.serviceId.serviceName}${node.instance}.${action}"
    } else {
        return "${prefix}#${node.serviceId.serviceName}.${action}"
    }
}

Task setup = project.tasks.create(name: taskName(prefix, node, 'clean'), type: Delete) {
    delete node.homeDir
    delete node.cwd
}

setup = project.tasks.create(name: taskName(prefix, node, 'createCwd'), type: DefaultTask, dependsOn: setup) {
    doLast {
        node.cwd.mkdirs()
    }
    outputs.dir node.cwd
}

setup = project.tasks.create(name: taskName(prefix, node, 'extract'), type: Copy, dependsOn: [setup, newVerifySha1]) {
    from {
        // FIXHERE: Need a way to get this package file from the node maybe?
        project.tarTree(project.resources.gzip(downloadHadoop.outputFile()))
    }
    into node.baseDir
}

Map nodeConfiguration = [
        'fs.defaultFS': 'hdfs://localhost:9000',
        'dfs.replication' : '1',
        'dfs.namenode.name.dir' : new File(node.dataDir, "dfs/name/").toURI().toString(),
        'dfs.datanode.data.dir' : new File(node.dataDir, "dfs/data/").toURI().toString(),
        'dfs.namenode.checkpoint.dir' : new File(node.dataDir, "dfs/namesecondary/").toURI().toString()
]

// Add all node level configs to node Configuration
setup = project.tasks.create(name: taskName(prefix, node, "writeConfig"), type: DefaultTask, dependsOn: setup) {
    doFirst {
        String properties = nodeConfiguration.collect { key, value ->
            "<property>\n\t\t<name>${key}</name>\n\t\t<value>${value}</value>\n\t</property>"
        }.join("\n\t")
        properties = "<configuration>\n\t${properties}\n</configuration>"
        node.configFile.setText(properties)
    }
}

/** Wrapper for command line argument: surrounds comma with double quotes **/
class EscapeCommaWrapper {
    Object arg

    public String toString() {
        String s = arg.toString()

        /// Surround strings that contains a comma with double quotes
        if (s.indexOf(',') != -1) {
            return "\"${s}\""
        }
        return s
    }
}

for (Map.Entry<String, Object[]> command : node.config.setupCommands(node.serviceId).entrySet()) {
    Object[] args = command.getValue().clone()
    final Object commandPath
    if (Os.isFamily(Os.FAMILY_WINDOWS)) {
        /*
         * We have to delay building the string as the path will not exist during configuration which will fail on
         * Windows due to getting the short name requiring the path to already exist. Note that we have to capture the
         * value of arg[0] now otherwise we would stack overflow later since arg[0] is replaced below.
         */
        String argsZero = args[0]
        commandPath = "${-> Paths.get(HadoopServiceInfo.getShortPathName(node.homeDir.toString())).resolve(argsZero.toString()).toString()}"
    } else {
        commandPath = node.homeDir.toPath().resolve(args[0].toString()).toString()
    }
    args[0] = commandPath
    setup = project.tasks.create(name: taskName(prefix, node, command.getKey()), type: LoggedExec, dependsOn: setup) { Exec exec ->
        exec.workingDir node.cwd
        exec.environment 'JAVA_HOME', node.getJavaHome()
        if (Os.isFamily(Os.FAMILY_WINDOWS)) {
            // fixhere eventually
            exec.executable 'cmd'
            exec.args '/C', 'call'
            exec.args args.collect { a -> new EscapeCommaWrapper(arg: a)}
        } else {
            exec.commandLine args
        }
    }
}

// Configure start task

// this closure is converted into ant nodes by groovy's AntBuilder
Closure antRunner = { AntBuilder ant ->
    ant.exec(executable: node.executable, spawn: node.spawn, dir: node.cwd, taskName: node.serviceId.serviceName) {
        node.env.each { key, value -> env(key: key, value: value) }
        node.args.each { arg(value: it) }
    }
}

// this closure is the actual code to run the node
Closure serviceRunner = {
    // Due to how ant exec works with the spawn option, we lose all stdout/stderr from the
    // process executed. To work around this, when spawning, we wrap the service start
    // command inside another shell script, which simply internally redirects the output
    // of the real start script. This allows ant to keep the streams open with the
    // dummy process, but us to have the output available if there is an error in the
    // services start script
    if (node.spawn) {
        node.writeWrapperScript()
    }

    node.getCommandString().eachLine { line -> logger.info(line) }

    if (logger.isInfoEnabled() || node.config.daemonize == false) {
        runAntCommand(project, antRunner, System.out, System.err)
    } else {
        // buffer the output, we may not need to print it
        PrintStream captureStream = new PrintStream(node.buffer, true, "UTF-8")
        runAntCommand(project, antRunner, captureStream, captureStream)
    }
}

Task start = project.tasks.create(name: taskName(prefix, node, 'start'), type: DefaultTask, dependsOn: setup)
// Do we need this?
//if (node.javaVersion != null) {
//    BuildPlugin.requireJavaHome(start, node.javaVersion)
//}
start.doLast(serviceRunner)
start.doFirst {
    // Configure HADOOP_OPTS (or similar env) - adds system properties, assertion flags, remote debug etc
    String envOptionKey = node.config.envSysPropOption(node.serviceId)
    List<String> hadoopJavaOpts = [node.env.get(envOptionKey, '')]
    String collectedSystemProperties = node.config.systemProperties.collect { key, value -> "-D${key}=${value}" }.join(" ")
    hadoopJavaOpts.add(collectedSystemProperties)
    hadoopJavaOpts.add(node.config.jvmArgs)
    if (Boolean.parseBoolean(System.getProperty('tests.asserts', 'true'))) {
        // put the enable assertions options before other options to allow
        // flexibility to disable assertions for specific packages or classes
        // in the cluster-specific options
        hadoopJavaOpts.add("-ea")
        hadoopJavaOpts.add("-esa")
    }
    // we must add debug options inside the closure so the config is read at execution time, as
    // gradle task options are not processed until the end of the configuration phase
    if (node.config.debug) {
        println "Running ${node.serviceId.serviceName} ${node.instance} in debug mode, " +
                "suspending until connected on port 8000"
        hadoopJavaOpts.add('-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8000')
    }
    node.env[envOptionKey] = hadoopJavaOpts.join(" ")

    project.logger.info("Starting ${node.serviceId.serviceName} in ${node.clusterName}")
}

// Configure daemon stop task

if (node.config.daemonize) {
    Task stop = project.tasks.create(name: taskName(prefix, node, 'stop'), type: LoggedExec, dependsOn: []) {
        onlyIf { node.pidDir.exists() && node.pidDir.listFiles().size() > 0 }
        // the pid file won't actually be read until execution time, since the read is wrapped within an inner closure of the GString
        ext.pid = "${ -> node.pidDir.listFiles().first().getText('UTF-8').trim()}"
        doFirst {
            project.logger.info("Shutting down external service with pid ${pid}")
        }
        if (Os.isFamily(Os.FAMILY_WINDOWS)) {
            executable 'Taskkill'
            args '/PID', pid, '/F'
        } else {
            executable 'kill'
            args '-9', pid
        }
        doLast {
            project.delete(node.pidDir)
        }
    }

    runner.finalizedBy(stop)
    start.finalizedBy(stop)
    for (Object dependency : config.dependencies) {
        if (dependency instanceof Fixture) {
            def depStop = ((Fixture)dependency).stopTask
            runner.finalizedBy(depStop)
            start.finalizedBy(depStop)
        }
    }
}

// End Configure Node call

startTasks.add(start)

// Configure Wait Task



// UTILITY FUNCTIONS:

/** Runs an ant command, sending output to the given out and error streams */
static Object runAntCommand(Project project, Closure command, PrintStream outputStream, PrintStream errorStream) {
    DefaultLogger listener = new DefaultLogger(
            errorPrintStream: errorStream,
            outputPrintStream: outputStream,
            messageOutputLevel: org.apache.tools.ant.Project.MSG_INFO)

    project.ant.project.addBuildListener(listener)
    Object retVal = command(project.ant)
    project.ant.project.removeBuildListener(listener)
    return retVal
}


























