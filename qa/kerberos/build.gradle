/*
 * Licensed to Elasticsearch under one or more contributor
 * license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright
 * ownership. Elasticsearch licenses this file to you under
 * the Apache License, Version 2.0 (the "License"); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */


import org.elasticsearch.gradle.test.AntFixture
import org.elasticsearch.hadoop.gradle.fixture.hadoop.ServiceDescriptor
import org.elasticsearch.hadoop.gradle.fixture.hadoop.conf.HadoopClusterConfiguration
import org.elasticsearch.hadoop.gradle.fixture.hadoop.HadoopClusterFormationTasks
import org.elasticsearch.hadoop.gradle.fixture.hadoop.conf.InstanceConfiguration
import org.elasticsearch.hadoop.gradle.fixture.hadoop.conf.ServiceConfiguration
import org.elasticsearch.hadoop.gradle.fixture.hadoop.conf.SettingsContainer
import org.elasticsearch.hadoop.gradle.fixture.hadoop.tasks.HadoopMRJob
import org.elasticsearch.hadoop.gradle.fixture.hadoop.tasks.HiveBeeline
import org.elasticsearch.hadoop.gradle.fixture.hadoop.tasks.PigScript
import org.elasticsearch.hadoop.gradle.fixture.hadoop.tasks.SparkApp

apply plugin: 'es.hadoop.build'
apply plugin: 'scala'

configurations {
    kdcFixture
}

dependencies {
    compile 'org.scala-lang:scala-library:2.11.8'
    compile project(":elasticsearch-spark-20")

    compile project(":elasticsearch-hadoop-mr")
    compile project(":elasticsearch-hadoop-mr").sourceSets.itest.runtimeClasspath

    kdcFixture project(':test:fixtures:minikdc')
}

String realm = '@BUILD.ELASTIC.CO'
String hostPart = '/build.elastic.co'

String esPrincipal = 'HTTP' + hostPart
String clientPrincipal = 'client' + hostPart
String namenodePrincipal = 'nn' + hostPart
String datanodePrincipal = 'dn' + hostPart
String resourceManagerPrincipal = 'rm' + hostPart
String nodeManagerPrincipal = 'nm' + hostPart
String historyServerPrincipal = 'jhs' + hostPart
String mapredPrincipal = 'mapred' + hostPart
String hivePrincipalName = 'hive' + hostPart

String esKeytabName = 'es.keytab'
String clientKeytabName = 'client.keytab'
String hadoopKeytabName = 'hadoop.keytab'
String hiveKeytabName = 'hive.keytab'

Map users = ["client":"password", "gmarx":"swordfish"]
Map<String, String> keytabUsers = [:]

keytabUsers.put(esPrincipal, esKeytabName)
keytabUsers.put(clientPrincipal, clientKeytabName)
keytabUsers.put('HTTP/hadoop.build.elastic.co', hadoopKeytabName)
keytabUsers.put(namenodePrincipal, hadoopKeytabName)
keytabUsers.put(datanodePrincipal, hadoopKeytabName)
keytabUsers.put(resourceManagerPrincipal, hadoopKeytabName)
keytabUsers.put(nodeManagerPrincipal, hadoopKeytabName)
keytabUsers.put(historyServerPrincipal, hadoopKeytabName)
keytabUsers.put(mapredPrincipal, hadoopKeytabName)
keytabUsers.put(hivePrincipalName, hiveKeytabName)

// Configure MiniKDC
AntFixture kdcFixture = project.tasks.create('kdcFixture', AntFixture) {
    dependsOn project.configurations.kdcFixture
    executable = new File(project.runtimeJavaHome, 'bin/java')
    env 'CLASSPATH', "${ -> project.configurations.kdcFixture.asPath }"
    waitCondition = { fixture, ant ->
        // the kdc wrapper writes the ports file when
        // it's ready, so we can just wait for the file to exist
        return fixture.portsFile.exists()
    }

    final List<String> kdcFixtureArgs = []

    // Add provisioning lines first for sys properties
    users.forEach { k, v -> kdcFixtureArgs.add("-Des.fixture.kdc.user.${k}.pwd=${v}") }
    keytabUsers.forEach { k, v -> kdcFixtureArgs.add("-Des.fixture.kdc.user.${k}.keytab=${v}") }

    // Common options
    kdcFixtureArgs.add('org.elasticsearch.hadoop.test.fixture.minikdc.MiniKdcFixture')
    kdcFixtureArgs.add(baseDir.toString())

    args kdcFixtureArgs.toArray()
}

// KDC Fixture artifacts
java.nio.file.Path fixtureDir = project.buildDir.toPath().resolve("fixtures").resolve("kdcFixture")
java.nio.file.Path krb5Conf = fixtureDir.resolve("krb5.conf")
java.nio.file.Path esKeytab = fixtureDir.resolve(esKeytabName)
java.nio.file.Path clientKeytab = fixtureDir.resolve(clientKeytabName)
java.nio.file.Path hadoopKeytab = fixtureDir.resolve(hadoopKeytabName)
java.nio.file.Path hiveKeytab = fixtureDir.resolve(hiveKeytabName)

// Configure ES with Kerberos Auth
esCluster {
    clusterConf {
        dependsOn(kdcFixture)

        // This may be needed if we ever run against java 9: --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED
//        systemProperty("java.security.krb5.conf", krb5Conf.toString())
        //systemProperty("sun.security.krb5.debug", "true")

        // force localhost IPv4 otherwise it is a chicken and egg problem where we need the keytab for the hostname when starting the cluster
        // but do not know the exact address that is first in the http ports file
//        setting 'http.host', '127.0.0.1'
//        setting 'xpack.license.self_generated.type', 'trial'
//        setting 'xpack.security.enabled', 'true'
//        setting 'xpack.security.authc.realms.file.type', 'file'
//        setting 'xpack.security.authc.realms.file.order', '0'
//        setting 'xpack.ml.enabled', 'false'
//        setting 'xpack.security.audit.enabled', 'true'
//        // Kerberos realm
//        setting 'xpack.security.authc.realms.kerberos.type', 'kerberos'
//        setting 'xpack.security.authc.realms.kerberos.order', '1'
//        setting 'xpack.security.authc.realms.kerberos.keytab.path', 'es.keytab'
//        setting 'xpack.security.authc.realms.kerberos.krb.debug', 'true'
//        setting 'xpack.security.authc.realms.kerberos.remove_realm_name', 'false'
//        // Token realm
//        setting 'xpack.security.authc.token.enabled', 'true'
//        setting 'xpack.security.authc.token.timeout', '20m'
//
//        setupCommand 'setupTestAdmin',
//                'bin/elasticsearch-users', 'useradd', "test_admin", '-p', 'x-pack-test-password', '-r', 'superuser'
//
//        extraConfigFile('es.keytab', esKeytab.toAbsolutePath())
//
//        // Override the wait condition; Do the same wait logic, but pass in the test credentials for the API
//        waitCondition = { node, ant ->
//            File tmpFile = new File(node.cwd, 'wait.success')
//            ant.get(src: "http://${node.httpUri()}/_cluster/health?wait_for_nodes=>=${numNodes}&wait_for_status=yellow",
//                    dest: tmpFile.toString(),
//                    username: 'test_admin',
//                    password: 'x-pack-test-password',
//                    ignoreerrors: true,
//                    retries: 10)
//            return tmpFile.exists()
//        }
    }
}

// Configure Integration Test Task
Test integrationTest = project.tasks.findByName('integrationTest') as Test
integrationTest.dependsOn(kdcFixture)
integrationTest.finalizedBy(kdcFixture.getStopTask())
integrationTest.systemProperty("java.security.krb5.conf", krb5Conf.toString())
//integrationTest.systemProperty("sun.security.krb5.debug", "true")
integrationTest.systemProperty("es.net.http.auth.user", "test_admin")
integrationTest.systemProperty("es.net.http.auth.pass", "x-pack-test-password")

// Need these for SSL items, test data, and scripts
File resourceDir = project.sourceSets.main.resources.getSrcDirs().head()
File mrItestResourceDir = project(":elasticsearch-hadoop-mr").sourceSets.itest.resources.getSrcDirs().head()

// =============================================================================
// Hadoop cluster testing
// =============================================================================

// Project instance available implicitly
String prefix = "hadoopFixture"
HadoopClusterConfiguration config = new HadoopClusterConfiguration(project, prefix)

// Hadoop cluster depends on KDC Fixture being up and running
config.addDependency(kdcFixture)

config.service('hadoop') { ServiceConfiguration s ->
    s.addSystemProperty("java.security.krb5.conf", krb5Conf.toString())
    // Core Site Config
    s.settingsFile('core-site.xml') { SettingsContainer.FileSettings f ->
        // Enable Security
        f.addSetting("hadoop.security.authentication", "kerberos")
        f.addSetting("hadoop.security.authorization", "true")
        f.addSetting("hadoop.rpc.protection", "authentication")
        f.addSetting("hadoop.ssl.require.client.cert", "false")
        f.addSetting("hadoop.ssl.hostname.verifier", "DEFAULT")
        f.addSetting("hadoop.ssl.keystores.factory.class", "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory")
        f.addSetting("hadoop.ssl.server.conf", "ssl-server.xml")
        f.addSetting("hadoop.ssl.client.conf", "ssl-client.xml")
        f.addSetting("hadoop.proxyuser.hive.hosts", "*")
        f.addSetting("hadoop.proxyuser.hive.groups", "*")
    }
    // SSL Server Config
    s.settingsFile('ssl-server.xml') { SettingsContainer.FileSettings f ->
        f.addSetting("ssl.server.keystore.type", "jks")
        f.addSetting("ssl.server.keystore.location", "${resourceDir.getAbsolutePath()}/ssl/server.jks")
        f.addSetting("ssl.server.keystore.password", "bigdata")
        f.addSetting("ssl.server.keystore.keypassword", "bigdata")
    }
    // HDFS Site Config
    s.settingsFile('hdfs-site.xml') { SettingsContainer.FileSettings f ->
        f.addSetting("dfs.http.policy", "HTTPS_ONLY")
        f.addSetting("dfs.web.authentication.kerberos.principal", "HTTP/hadoop.build.elastic.co$realm")
        f.addSetting("dfs.web.authentication.kerberos.keytab", "$hadoopKeytab")
        f.addSetting("dfs.block.access.token.enable", "true")
        f.addSetting("dfs.namenode.kerberos.principal", "$namenodePrincipal$realm")
        f.addSetting("dfs.namenode.keytab.file", "$hadoopKeytab")
        f.addSetting("dfs.namenode.kerberos.internal.spnego.principal", "HTTP/hadoop.build.elastic.co")
        f.addSetting("dfs.datanode.data.dir.perm", "700")
        f.addSetting("dfs.datanode.kerberos.principal", "$datanodePrincipal$realm")
        f.addSetting("dfs.datanode.keytab.file", "$hadoopKeytab")
        f.addSetting("dfs.encrypt.data.transfer", "false")
        f.addSetting("dfs.data.transfer.protection", "authentication")
    }
    // Yarn Site Config
    s.settingsFile('yarn-site.xml') { SettingsContainer.FileSettings f ->
        f.addSetting("yarn.resourcemanager.principal", "$resourceManagerPrincipal$realm")
        f.addSetting("yarn.resourcemanager.keytab", "$hadoopKeytab")
        f.addSetting("yarn.nodemanager.principal", "$nodeManagerPrincipal$realm")
        f.addSetting("yarn.nodemanager.keytab", "$hadoopKeytab")
    }
    // Mapred Site Config
    s.settingsFile('mapred-site.xml') { SettingsContainer.FileSettings f ->
        f.addSetting("mapreduce.framework.name", "yarn")
        f.addSetting("mapreduce.shuffle.ssl.enabled", "false")
        f.addSetting("mapreduce.jobhistory.principal", "$historyServerPrincipal$realm")
        f.addSetting("mapreduce.jobhistory.keytab", "$hadoopKeytab")
        f.addSetting("yarn.resourcemanager.principal", "$resourceManagerPrincipal$realm")
    }
}
config.service('spark')
config.service('hive') { ServiceConfiguration s ->
    s.addSystemProperty("java.security.krb5.conf", krb5Conf.toString())
    s.addSetting('hive.server2.authentication', 'kerberos')
    s.addSetting('hive.server2.authentication.kerberos.principal', "$hivePrincipalName$realm")
    s.addSetting('hive.server2.authentication.kerberos.keytab', "$hiveKeytab")
    s.addSetting('yarn.app.mapreduce.am.command-opts', "-Djava.security.krb5.conf=${krb5Conf.toString()}")
    s.addSetting('mapreduce.map.java.opts', "-Djava.security.krb5.conf=${krb5Conf.toString()}")
    s.addSetting('mapreduce.reduce.java.opts', "-Djava.security.krb5.conf=${krb5Conf.toString()}")
}
config.service('pig') { ServiceConfiguration s ->
    s.addSetting('java.security.krb5.conf', krb5Conf.toString())
    s.addSetting('hadoop.security.krb5.principal', "$clientPrincipal$realm")
    s.addSetting('hadoop.security.krb5.keytab', clientKeytab.toString())
    s.addSetting('yarn.app.mapreduce.am.command-opts', "-Djava.security.krb5.conf=${krb5Conf.toString()}")
    s.addSetting('mapreduce.map.java.opts', "-Djava.security.krb5.conf=${krb5Conf.toString()}")
    s.addSetting('mapreduce.reduce.java.opts', "-Djava.security.krb5.conf=${krb5Conf.toString()}")
}

// Hadoop cluster should just depend on the jar and test jar artifacts
def jar = project.tasks.getByName('jar') as org.gradle.jvm.tasks.Jar
def testingJar = project.rootProject.tasks.findByName('hadoopTestingJar') as Jar
config.addDependency(jar)
config.addDependency(testingJar)

// Create a task to use to leave the cluster online until a user provides input to tear it down.
Task testCluster = project.tasks.create("testCluster")
testCluster.doLast {
    project.logger.lifecycle("TEST FINISHED")
    System.console().readLine("Tear Down? ")
    project.logger.lifecycle("STOPPING CLUSTER")
}
esCluster.addTask(testCluster)
config.addClusterTask(testCluster)

// We need to create a tmp directory in hadoop before history server does, because history server will set permissions
// wrong.
HadoopMRJob createTmp = config.service('hadoop').role('datanode').createClusterTask('createTmp', HadoopMRJob.class) {
    clusterConfiguration = config
    // Run on namenode since the gateway is not yet set up
    runOn(config.service('hadoop').role('namenode').instance(0))
    dependsOn(jar)
    jobJar = jar.archivePath
    jobClass = 'org.elasticsearch.hadoop.qa.kerberos.dfs.SecureFsShell'
    systemProperties([
            "test.krb5.principal": namenodePrincipal,
            "test.krb5.keytab": hadoopKeytab.toString(),
    ])
    args = ['-mkdir', '-p', '/tmp/hadoop-yarn/staging/history']
}
HadoopMRJob prepareTmp = config.service('hadoop').role('datanode').createClusterTask('prepareTmp', HadoopMRJob.class) {
    clusterConfiguration = config
    // Run on namenode since the gateway is not yet set up
    runOn(config.service('hadoop').role('namenode').instance(0))
    dependsOn(jar, createTmp)
    jobJar = jar.archivePath
    jobClass = 'org.elasticsearch.hadoop.qa.kerberos.dfs.SecureFsShell'
    systemProperties([
            "test.krb5.principal": namenodePrincipal,
            "test.krb5.keytab": hadoopKeytab.toString(),
    ])
    // Recursive should be fine here since it's before anything is on the FS
    args = ['-chmod', '-R', '777', '/tmp']
}
testCluster.dependsOn(prepareTmp)
config.service('hadoop').role('historyserver').addDependency(prepareTmp)
config.service('hive').addDependency(prepareTmp)

// We must create the data directory before copying test data. DfsCopy task would normally do this
// automatically, but we have to wrap the fs shell for Kerberos.
HadoopMRJob createDataDir = config.createClusterTask('createDataDir', HadoopMRJob.class) {
    clusterConfiguration = config
    executedOn = config.service('hadoop').role('namenode').instance(0)
    dependsOn(jar)
    jobJar = jar.archivePath
    jobClass = 'org.elasticsearch.hadoop.qa.kerberos.dfs.SecureFsShell'
    systemProperties([
            "test.krb5.principal": clientPrincipal,
            "test.krb5.keytab": clientKeytab.toString(),
            "java.security.krb5.conf": krb5Conf.toString()
    ])
    args = ['-mkdir', '-p', '/data/artists/']
}
testCluster.dependsOn(createDataDir)

// Copy the test data to HDFS using the SecureFsShell instead of dfs copy. We could use the
// DfsCopy task, but with Kerberos, we would have to kinit before running it. Instead we wrap.
HadoopMRJob copyData = config.createClusterTask('copyData', HadoopMRJob.class) {
    clusterConfiguration = config
    dependsOn(createDataDir, jar)
    jobJar = jar.archivePath
    jobClass = 'org.elasticsearch.hadoop.qa.kerberos.dfs.SecureFsShell'
    systemProperties([
            "test.krb5.principal": clientPrincipal,
            "test.krb5.keytab": clientKeytab.toString(),
            "java.security.krb5.conf": krb5Conf.toString()
    ])
    environmentVariables.put('HADOOP_ROOT_LOGGER','TRACE,console')
    args = ['-copyFromLocal', project.file(new File(mrItestResourceDir, 'artists.dat')), "/data/artists/artists.dat"]
}
testCluster.dependsOn(copyData)

// Run the MR job to load data to ES. Ensure Kerberos settings are available.
HadoopMRJob mrLoadData = config.createClusterTask('mrLoadData', HadoopMRJob.class) {
    clusterConfiguration = config
    dependsOn(copyData)
    jobJar = jar.archivePath
    libJars.add(testingJar.archivePath)
    jobClass = 'org.elasticsearch.hadoop.qa.kerberos.mr.LoadToES'
    jobSettings = [
            'es.resource': 'qa_kerberos_mr_data/_doc',
            'es.nodes': 'localhost:9500',
            'load.field.names': 'number,name,url,picture,@timestamp,tag',
            'mapreduce.map.java.opts': "-Djava.security.krb5.conf=${krb5Conf.toString()}",
            'mapreduce.reduce.java.opts': "-Djava.security.krb5.conf=${krb5Conf.toString()}",
            'yarn.app.mapreduce.am.command-opts': "-Djava.security.krb5.conf=${krb5Conf.toString()}"
    ]
    systemProperties([
            "test.krb5.principal": clientPrincipal,
            "test.krb5.keytab": clientKeytab.toString(),
            "java.security.krb5.conf": krb5Conf.toString()
    ])
    args = ['/data/artists/artists.dat']
}
esCluster.addTask(mrLoadData)
testCluster.dependsOn(mrLoadData)

// Run the MR job to read data out of ES. Ensure Kerberos settings are available.
HadoopMRJob mrReadData = config.createClusterTask('mrReadData', HadoopMRJob.class) {
    clusterConfiguration = config
    dependsOn(mrLoadData)
    jobJar = jar.archivePath
    libJars.add(testingJar.archivePath)
    jobClass = 'org.elasticsearch.hadoop.qa.kerberos.mr.ReadFromES'
    jobSettings = [
            'es.resource': 'qa_kerberos_mr_data/_doc',
            'es.nodes': 'localhost:9500',
            'mapreduce.map.java.opts': "-Djava.security.krb5.conf=${krb5Conf.toString()}",
            'mapreduce.reduce.java.opts': "-Djava.security.krb5.conf=${krb5Conf.toString()}",
            'yarn.app.mapreduce.am.command-opts': "-Djava.security.krb5.conf=${krb5Conf.toString()}"
    ]
    systemProperties([
            "test.krb5.principal": clientPrincipal,
            "test.krb5.keytab": clientKeytab.toString(),
            "java.security.krb5.conf": krb5Conf.toString()
    ])
    args = ['/data/output/mr']
}
esCluster.addTask(mrReadData)
testCluster.dependsOn(mrReadData)

File outputDataDir = project.file(new File(project.buildDir, 'data'))
Task createOutputDataDir = project.tasks.create("createOutputDataDir") {
    doLast {
        if (outputDataDir.exists()) {
            // Clean out any prior results
            outputDataDir.deleteDir()
        }
        outputDataDir.mkdirs()
    }
}

// Copy the test data to HDFS using the SecureFsShell instead of dfs copy. We could use the
// DfsCopy task, but with Kerberos, we would have to kinit before running it. Instead we wrap.
HadoopMRJob copyMrOutput = config.createClusterTask('copyMrOutput', HadoopMRJob.class) {
    clusterConfiguration = config
    dependsOn(mrReadData, createOutputDataDir)
    jobJar = jar.archivePath
    jobClass = 'org.elasticsearch.hadoop.qa.kerberos.dfs.SecureFsShell'
    systemProperties([
            "test.krb5.principal": clientPrincipal,
            "test.krb5.keytab": clientKeytab.toString(),
            "java.security.krb5.conf": krb5Conf.toString()
    ])
    environmentVariables.put('HADOOP_ROOT_LOGGER','TRACE,console')
    args = ['-copyToLocal', "/data/output/mr", outputDataDir]
}
testCluster.dependsOn(copyMrOutput)

// Run the Spark job to load data to ES. Ensure Kerberos settings are available.
SparkApp sparkLoadData = config.createClusterTask('sparkLoadData', SparkApp.class) {
    clusterConfiguration = config
    dependsOn(copyData)
//    deployModeCluster()
//    principal = clientPrincipal + realm
//    keytab = clientKeytab.toString()
    jobJar = jar.archivePath
    libJars.add(testingJar.archivePath)
    jobClass = 'org.elasticsearch.hadoop.qa.kerberos.spark.LoadToES'
    jobSettings([
            'spark.es.resource': 'qa_kerberos_spark_data/_doc',
            'spark.es.nodes': 'localhost:9500',
            'spark.load.field.names': 'number,name,url,picture,@timestamp,tag',
            'spark.yarn.am.extraJavaOptions': "-Djava.security.krb5.conf=${krb5Conf.toString()}",
            'spark.driver.extraJavaOptions': "-Djava.security.krb5.conf=${krb5Conf.toString()}",
            'spark.executor.extraJavaOptions': "-Djava.security.krb5.conf=${krb5Conf.toString()}"
    ])
    env.put('SPARK_SUBMIT_OPTS', "-Djava.security.krb5.conf=${krb5Conf.toString()} " +
            "-Dtest.krb5.principal=$clientPrincipal$realm " +
            "-Dtest.krb5.keytab=${clientKeytab.toString()}")
    args = ['/data/artists/artists.dat']
}
esCluster.addTask(sparkLoadData)
testCluster.dependsOn(sparkLoadData)

// Run the Spark job to load data to ES. Ensure Kerberos settings are available.
SparkApp sparkReadData = config.createClusterTask('sparkReadData', SparkApp.class) {
    clusterConfiguration = config
    dependsOn(sparkLoadData)
//    deployModeCluster()
//    principal = clientPrincipal + realm
//    keytab = clientKeytab.toString()
    jobJar = jar.archivePath
    libJars.add(testingJar.archivePath)
    jobClass = 'org.elasticsearch.hadoop.qa.kerberos.spark.ReadFromES'
    jobSettings([
            'spark.es.resource': 'qa_kerberos_spark_data/_doc',
            'spark.es.nodes': 'localhost:9500',
            'spark.yarn.am.extraJavaOptions': "-Djava.security.krb5.conf=${krb5Conf.toString()}",
            'spark.driver.extraJavaOptions': "-Djava.security.krb5.conf=${krb5Conf.toString()}",
            'spark.executor.extraJavaOptions': "-Djava.security.krb5.conf=${krb5Conf.toString()}"
    ])
    env.put('SPARK_SUBMIT_OPTS', "-Djava.security.krb5.conf=${krb5Conf.toString()} " +
            "-Dtest.krb5.principal=$clientPrincipal$realm " +
            "-Dtest.krb5.keytab=${clientKeytab.toString()}")
    args = ['/data/output/spark']
}
esCluster.addTask(sparkReadData)
testCluster.dependsOn(sparkReadData)

// Copy the test data to HDFS using the SecureFsShell instead of dfs copy. We could use the
// DfsCopy task, but with Kerberos, we would have to kinit before running it. Instead we wrap.
HadoopMRJob copySparkOutput = config.createClusterTask('copySparkOutput', HadoopMRJob.class) {
    clusterConfiguration = config
    dependsOn(sparkReadData, createOutputDataDir)
    jobJar = jar.archivePath
    jobClass = 'org.elasticsearch.hadoop.qa.kerberos.dfs.SecureFsShell'
    systemProperties([
            "test.krb5.principal": clientPrincipal,
            "test.krb5.keytab": clientKeytab.toString(),
            "java.security.krb5.conf": krb5Conf.toString()
    ])
    environmentVariables.put('HADOOP_ROOT_LOGGER','TRACE,console')
    args = ['-copyToLocal', "/data/output/spark", outputDataDir]
}
testCluster.dependsOn(copySparkOutput)

// Replace the regular beeline script with our own shimmed one. All we do is perform a keytab login and
// delegate on regularly. This is easier than re-implementing most of beeline.
Copy patchBeeline = config.createClusterTask('patchBeeline', Copy.class) {
    InstanceConfiguration i = config.service('hive').role('hiveserver').instance(0)
    ServiceDescriptor hive = i.getServiceDescriptor()
    Object lazyEvalBeelineScript = "${->i.getBaseDir().toPath().resolve(hive.homeDirName(i)).resolve(hive.scriptDir(i)).resolve('ext').toString()}"
    String patchedBeeline = resourceDir.toPath().resolve('hive').resolve('patches').resolve('1.2.2').resolve('beeline.sh').toString()
    // Just in case any of the directories in Hive are shifted around, before running the fixture, we resolve
    // the path to the beeline ext script lazily.
    from patchedBeeline
    into lazyEvalBeelineScript
    setDuplicatesStrategy(DuplicatesStrategy.INCLUDE)
}

HiveBeeline hiveLoadData = config.createClusterTask('hiveLoadData', HiveBeeline.class) {
    clusterConfiguration = config
    dependsOn(jar, copyData, patchBeeline)
    hivePrincipal = hivePrincipalName + realm
    script = new File(resourceDir, 'hive/load_to_es.sql')
    libJars.add(testingJar.archivePath)
    env.putAll([
            'HADOOP_CLIENT_OPTS':
                    "-Djava.security.krb5.conf=${krb5Conf.toString()} " +
                    "-Dtest.krb5.principal=$clientPrincipal$realm " +
                    "-Dtest.krb5.keytab=${clientKeytab.toString()} ",
            'TEST_LIB': jar.archivePath.toString()
    ])
}
esCluster.addTask(hiveLoadData)
testCluster.dependsOn(hiveLoadData)

PigScript pigLoadData = config.createClusterTask('pigLoadData', PigScript.class) {
    clusterConfiguration = config
    dependsOn(copyData)
    script = new File(resourceDir, 'pig/load_to_es.pig')
    libJars.add(testingJar.archivePath)
    env = [
            'PIG_OPTS': "-Djava.security.krb5.conf=${krb5Conf.toString()}"
    ]
}
esCluster.addTask(pigLoadData)
testCluster.dependsOn(pigLoadData)

HadoopClusterFormationTasks.setup(project, config)
